{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLDZgQGzCxvY"
      },
      "source": [
        "**Students**  \n",
        "Brendan Hendricks  \n",
        "Rizvee Ahmed  \n",
        "Pranit Yadav  \n",
        "Shahmir Javed  \n",
        "Nathan Arimilli  \n",
        "Sreejony Sengupta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIX2mnXQ8ojZ",
        "outputId": "34572158-f9ca-4561-94d6-ad8afc3583b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2xtcR0aCWJ8",
        "outputId": "5f8fd0b6-46c8-48e4-be01-5b80e6a99cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com] [1 InRelease 14.2 kB/129 kB 11%] [Waiting\r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [1 InRelease 14.2 kB/129 kB 11%] [Waiting\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [81.0 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,371 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,803 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,609 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,305 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,577 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,690 kB]\n",
            "Fetched 28.1 MB in 3s (10.7 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 32.5 MB of archives.\n",
            "After this operation, 130 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.16 [76.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.16 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.68.5+ubuntu22.04.1 [30.0 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.6 [3,668 B]\n",
            "Fetched 32.5 MB in 2s (14.0 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126441 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.16) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126641 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.68.5+ubuntu22.04.1_amd64.deb ...\n",
            "Unpacking snapd (2.68.5+ubuntu22.04.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.16) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.68.5+ubuntu22.04.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126868 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.6_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.6) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.6) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.16) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Collecting typing_extensions~=4.14.0 (from selenium)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, fake-useragent, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "Successfully installed fake-useragent-2.2.0 outcome-1.3.0.post0 selenium-4.35.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 wsproto-1.2.0\n",
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.12/dist-packages (4.35.0)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.12/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium beautifulsoup4 pandas fake-useragent requests lxml\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install gensim\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HakrEuXDCiEW"
      },
      "source": [
        "Task A: Extract 8-10k reviews. However, many reviews may not have any text and will therefore be\n",
        "discarded. So be prepared to scrape a lot of pages of reviews per beer. Beeradvocate.com requires you\n",
        "to log in to read reviews beyond the first page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKgJJBRnbbuv"
      },
      "source": [
        "WebScraper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "import tempfile\n",
        "from urllib.parse import urljoin\n",
        "from fake_useragent import UserAgent"
      ],
      "metadata": {
        "id": "xvhgV7tZGXgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RYyExZa_bfdx",
        "outputId": "1e2e0dd1-5896-4c0f-c901-9998b5c3ef8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV files initialized\n",
            "Setup complete\n",
            "Getting beer list...\n",
            "Fetching beer list...\n",
            "Error: 403 Client Error: Forbidden for url: https://www.beeradvocate.com/beer/top-rated/\n",
            "No beers found\n",
            "Got 0 beers\n",
            "Chrome driver setup failed: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#sessionnotcreatedexception\n",
            "Stacktrace:\n",
            "#0 0x5580ef15afba <unknown>\n",
            "#1 0x5580eebdf6d0 <unknown>\n",
            "#2 0x5580eec1a475 <unknown>\n",
            "#3 0x5580eec16368 <unknown>\n",
            "#4 0x5580eec65280 <unknown>\n",
            "#5 0x5580eec64946 <unknown>\n",
            "#6 0x5580eec56c03 <unknown>\n",
            "#7 0x5580eec237a8 <unknown>\n",
            "#8 0x5580eec24421 <unknown>\n",
            "#9 0x5580ef11fb28 <unknown>\n",
            "#10 0x5580ef12387f <unknown>\n",
            "#11 0x5580ef107c49 <unknown>\n",
            "#12 0x5580ef124405 <unknown>\n",
            "#13 0x5580ef0ed4ff <unknown>\n",
            "#14 0x5580ef148258 <unknown>\n",
            "#15 0x5580ef148432 <unknown>\n",
            "#16 0x5580ef159fa3 <unknown>\n",
            "#17 0x7897c5e28ac3 <unknown>\n",
            "\n",
            "Trying minimal fallback setup...\n"
          ]
        },
        {
          "ename": "SessionNotCreatedException",
          "evalue": "Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#sessionnotcreatedexception\nStacktrace:\n#0 0x562f96738fba <unknown>\n#1 0x562f961bd6d0 <unknown>\n#2 0x562f961f8475 <unknown>\n#3 0x562f961f4368 <unknown>\n#4 0x562f96243280 <unknown>\n#5 0x562f96242946 <unknown>\n#6 0x562f96234c03 <unknown>\n#7 0x562f962017a8 <unknown>\n#8 0x562f96202421 <unknown>\n#9 0x562f966fdb28 <unknown>\n#10 0x562f9670187f <unknown>\n#11 0x562f966e5c49 <unknown>\n#12 0x562f96702405 <unknown>\n#13 0x562f966cb4ff <unknown>\n#14 0x562f96726258 <unknown>\n#15 0x562f96726432 <unknown>\n#16 0x562f96737fa3 <unknown>\n#17 0x7d7388ef1ac3 <unknown>\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2577600737.py\u001b[0m in \u001b[0;36msetup_chrome_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchrome_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mbrowser_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command_executor, keep_alive, file_detector, options, locator_converter, web_element_cls, client_config)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fedcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFedCM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mstart_session\u001b[0;34m(self, capabilities)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEW_SESSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#sessionnotcreatedexception\nStacktrace:\n#0 0x5580ef15afba <unknown>\n#1 0x5580eebdf6d0 <unknown>\n#2 0x5580eec1a475 <unknown>\n#3 0x5580eec16368 <unknown>\n#4 0x5580eec65280 <unknown>\n#5 0x5580eec64946 <unknown>\n#6 0x5580eec56c03 <unknown>\n#7 0x5580eec237a8 <unknown>\n#8 0x5580eec24421 <unknown>\n#9 0x5580ef11fb28 <unknown>\n#10 0x5580ef12387f <unknown>\n#11 0x5580ef107c49 <unknown>\n#12 0x5580ef124405 <unknown>\n#13 0x5580ef0ed4ff <unknown>\n#14 0x5580ef148258 <unknown>\n#15 0x5580ef148432 <unknown>\n#16 0x5580ef159fa3 <unknown>\n#17 0x7897c5e28ac3 <unknown>\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2577600737.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;31m# Setup driver and login\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_chrome_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0mlogged_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogin_to_beeradvocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2577600737.py\u001b[0m in \u001b[0;36msetup_chrome_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mchrome_options_simple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'--user-data-dir={temp_dir2}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchrome_options_simple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✓ Fallback Chrome driver setup successful\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mbrowser_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mvendor_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"goog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command_executor, keep_alive, file_detector, options, locator_converter, web_element_cls, client_config)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authenticator_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fedcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFedCM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mstart_session\u001b[0;34m(self, capabilities)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_caps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEW_SESSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"capabilities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alert\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#sessionnotcreatedexception\nStacktrace:\n#0 0x562f96738fba <unknown>\n#1 0x562f961bd6d0 <unknown>\n#2 0x562f961f8475 <unknown>\n#3 0x562f961f4368 <unknown>\n#4 0x562f96243280 <unknown>\n#5 0x562f96242946 <unknown>\n#6 0x562f96234c03 <unknown>\n#7 0x562f962017a8 <unknown>\n#8 0x562f96202421 <unknown>\n#9 0x562f966fdb28 <unknown>\n#10 0x562f9670187f <unknown>\n#11 0x562f966e5c49 <unknown>\n#12 0x562f96702405 <unknown>\n#13 0x562f966cb4ff <unknown>\n#14 0x562f96726258 <unknown>\n#15 0x562f96726432 <unknown>\n#16 0x562f96737fa3 <unknown>\n#17 0x7d7388ef1ac3 <unknown>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "BASE_URL = \"https://www.beeradvocate.com\"\n",
        "TOP_BEERS_URL = \"https://www.beeradvocate.com/beer/top-rated/\"\n",
        "CSV_FILENAME = 'beeradvocate_reviews.csv'\n",
        "PRODUCTS_FILENAME = 'beer_products_list.csv'\n",
        "MAX_BEERS = 500  # Increased to ensure we get enough\n",
        "TARGET_QUALITY_REVIEWS = 10000  # Target number of actual quality reviews\n",
        "MIN_REVIEW_LENGTH = 50  # Minimum characters for quality review\n",
        "MAX_PAGES_PER_BEER = 8  # Pages to scrape per beer\n",
        "\n",
        "# Login credentials\n",
        "BA_USERNAME = \"toadlover4\"\n",
        "BA_PASSWORD = \"RizveeBrendan\"\n",
        "\n",
        "def setup_chrome_driver():\n",
        "    chrome_options = Options()\n",
        "\n",
        "    # Essential flags for Colab/Jupyter\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument('--disable-gpu')\n",
        "\n",
        "    # Create unique temp directory to avoid conflicts\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    chrome_options.add_argument(f'--user-data-dir={temp_dir}')\n",
        "\n",
        "    # Anti-detection\n",
        "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "    ua = UserAgent()\n",
        "    chrome_options.add_argument(f'--user-agent={ua.random}')\n",
        "\n",
        "    try:\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "        print(\"✓ Chrome driver setup successful\")\n",
        "        return driver\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Chrome driver setup failed: {e}\")\n",
        "        print(\"Trying minimal fallback setup...\")\n",
        "\n",
        "        # Minimal fallback\n",
        "        chrome_options_simple = Options()\n",
        "        chrome_options_simple.add_argument('--headless')\n",
        "        chrome_options_simple.add_argument('--no-sandbox')\n",
        "        chrome_options_simple.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "        temp_dir2 = tempfile.mkdtemp()\n",
        "        chrome_options_simple.add_argument(f'--user-data-dir={temp_dir2}')\n",
        "\n",
        "        driver = webdriver.Chrome(options=chrome_options_simple)\n",
        "        print(\"✓ Fallback Chrome driver setup successful\")\n",
        "        return driver\n",
        "\n",
        "def login_to_beeradvocate(driver):\n",
        "    \"\"\"Login to BeerAdvocate to access additional review pages\"\"\"\n",
        "    try:\n",
        "        print(\"Logging into BeerAdvocate...\")\n",
        "        driver.get(\"https://www.beeradvocate.com/community/login/\")\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Find and fill username\n",
        "        try:\n",
        "            username_field = driver.find_element(By.NAME, \"login\")\n",
        "            username_field.clear()\n",
        "            username_field.send_keys(BA_USERNAME)\n",
        "        except:\n",
        "            print(\"Could not find username field\")\n",
        "            return False\n",
        "\n",
        "        # Find and fill password\n",
        "        try:\n",
        "            password_field = driver.find_element(By.NAME, \"password\")\n",
        "            password_field.clear()\n",
        "            password_field.send_keys(BA_PASSWORD)\n",
        "        except:\n",
        "            print(\"Could not find password field\")\n",
        "            return False\n",
        "\n",
        "        # Submit form\n",
        "        try:\n",
        "            login_button = driver.find_element(By.CSS_SELECTOR, \"input[type='submit']\")\n",
        "            login_button.click()\n",
        "            time.sleep(3)\n",
        "        except:\n",
        "            print(\"Could not find login button\")\n",
        "            return False\n",
        "\n",
        "        # Check if login was successful\n",
        "        current_url = driver.current_url\n",
        "        page_source = driver.page_source.lower()\n",
        "\n",
        "        if \"logout\" in page_source or \"welcome back\" in page_source or \"community\" in current_url:\n",
        "            print(\"✓ Successfully logged in!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"✗ Login may have failed - continuing anyway\")\n",
        "            return True  # Continue even if unsure\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Login error: {e} - continuing without login\")\n",
        "        return True  # Continue anyway\n",
        "\n",
        "def init_csv_files():\n",
        "    with open(CSV_FILENAME, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['product_name', 'product_review', 'user_rating'])\n",
        "\n",
        "    with open(PRODUCTS_FILENAME, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['beer_name', 'url'])\n",
        "\n",
        "    print(\"CSV files initialized\")\n",
        "\n",
        "def extract_beer_list():\n",
        "    print(\"Fetching beer list...\")\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': UserAgent().random,\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5',\n",
        "        'Connection': 'keep-alive',\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(TOP_BEERS_URL, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        print(f\"Got page: {response.status_code}\")\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        beer_links = soup.find_all('a', href=re.compile(r'/beer/profile/\\d+/\\d+'))\n",
        "\n",
        "        if not beer_links:\n",
        "            beer_links = soup.find_all('a', href=re.compile(r'/beer/'))\n",
        "            beer_links = [link for link in beer_links if 'profile' in link.get('href', '')]\n",
        "\n",
        "        print(f\"Found {len(beer_links)} beer links\")\n",
        "\n",
        "        beers = []\n",
        "        seen_urls = set()\n",
        "\n",
        "        for link in beer_links[:MAX_BEERS]:\n",
        "            href = link.get('href')\n",
        "            if not href:\n",
        "                continue\n",
        "\n",
        "            beer_url = urljoin(BASE_URL, href)\n",
        "            if beer_url in seen_urls:\n",
        "                continue\n",
        "            seen_urls.add(beer_url)\n",
        "\n",
        "            beer_name = link.text.strip()\n",
        "            if not beer_name or len(beer_name) < 3:\n",
        "                continue\n",
        "\n",
        "            beers.append({\n",
        "                'beer_name': beer_name,\n",
        "                'url': beer_url\n",
        "            })\n",
        "\n",
        "        print(f\"Collected {len(beers)} beers\")\n",
        "        return beers\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_reviews_from_beer_multipage(driver, beer_info):\n",
        "    \"\"\"Extract quality reviews from multiple pages of a beer - ONLY COUNT QUALITY ONES\"\"\"\n",
        "    beer_url = beer_info['url']\n",
        "    beer_name = beer_info['beer_name']\n",
        "    quality_reviews = []  # Only store reviews that pass quality filter\n",
        "\n",
        "    try:\n",
        "        # Get base beer page first\n",
        "        driver.get(beer_url)\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Try to find reviews section or construct URL\n",
        "        base_reviews_url = beer_url\n",
        "\n",
        "        # Look for reviews link patterns\n",
        "        try:\n",
        "            # Common BeerAdvocate review URL patterns\n",
        "            match = re.search(r'/beer/profile/(\\d+)/(\\d+)', beer_url)\n",
        "            if match:\n",
        "                brewery_id, beer_id = match.groups()\n",
        "                base_reviews_url = f\"https://www.beeradvocate.com/beer/profile/{brewery_id}/{beer_id}/\"\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Scrape multiple pages\n",
        "        for page in range(MAX_PAGES_PER_BEER):\n",
        "            try:\n",
        "                # Construct page URL - BeerAdvocate uses ?start= parameter\n",
        "                if page == 0:\n",
        "                    page_url = base_reviews_url\n",
        "                else:\n",
        "                    page_url = f\"{base_reviews_url}?start={25 * page}\"\n",
        "\n",
        "                driver.get(page_url)\n",
        "                time.sleep(1.5)\n",
        "\n",
        "                # Extract reviews from current page\n",
        "                page_quality_reviews = extract_quality_reviews_from_current_page(driver, beer_name)\n",
        "\n",
        "                if not page_quality_reviews:\n",
        "                    print(f\"     No quality reviews on page {page+1}, stopping\")\n",
        "                    break\n",
        "\n",
        "                quality_reviews.extend(page_quality_reviews)\n",
        "                print(f\"     Page {page+1}: +{len(page_quality_reviews)} quality reviews\")\n",
        "\n",
        "                # Stop if we have enough from this beer\n",
        "                if len(quality_reviews) >= 80:  # Reasonable limit per beer\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"     Error on page {page+1}: {e}\")\n",
        "                break\n",
        "\n",
        "        return quality_reviews\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Error: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_quality_reviews_from_current_page(driver, beer_name):\n",
        "    \"\"\"Extract ONLY quality reviews from current page - strict filtering\"\"\"\n",
        "    quality_reviews = []\n",
        "\n",
        "    try:\n",
        "        # BeerAdvocate-specific selectors\n",
        "        review_selectors = [\n",
        "            'div[id^=\"rating_\"]',  # Primary: BeerAdvocate uses rating_XXXXX\n",
        "            'tr[id^=\"rating_\"]',   # Alternative table format\n",
        "            'div.review-content',  # Sometimes uses this class\n",
        "            'tr[bgcolor]',         # Colored table rows\n",
        "            'table tr'             # Fallback\n",
        "        ]\n",
        "\n",
        "        review_elements = []\n",
        "        for selector in review_selectors:\n",
        "            elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "            if len(elements) >= 5:  # Found reasonable number\n",
        "                review_elements = elements\n",
        "                break\n",
        "\n",
        "        # Process each element\n",
        "        for element in review_elements[:50]:  # Limit per page\n",
        "            try:\n",
        "                review_data = extract_single_quality_review(element, beer_name)\n",
        "                if review_data:  # Only add if it passes quality check\n",
        "                    quality_reviews.append(review_data)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        return quality_reviews\n",
        "\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "def extract_single_quality_review(review_elem, beer_name):\n",
        "    \"\"\"Extract single review with STRICT quality filtering\"\"\"\n",
        "    try:\n",
        "        # Get text content\n",
        "        full_text = \"\"\n",
        "        if hasattr(review_elem, 'text'):\n",
        "            full_text = review_elem.text\n",
        "        else:\n",
        "            full_text = review_elem.get_attribute('textContent') or \"\"\n",
        "\n",
        "        if len(full_text.strip()) < MIN_REVIEW_LENGTH:\n",
        "            return None  # Too short\n",
        "\n",
        "        # Extract rating\n",
        "        rating = extract_rating_from_text(full_text)\n",
        "\n",
        "        # Extract and clean review text\n",
        "        review_text = extract_clean_review_text(full_text)\n",
        "\n",
        "        # QUALITY FILTERS - all must pass\n",
        "        if not review_text or len(review_text) < MIN_REVIEW_LENGTH:\n",
        "            return None\n",
        "\n",
        "        if not has_beer_content(review_text):\n",
        "            return None\n",
        "\n",
        "        if len(review_text.split()) < 10:  # At least 10 words\n",
        "            return None\n",
        "\n",
        "        # Must have meaningful content\n",
        "        if is_meaningless_text(review_text):\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            'product_name': beer_name,\n",
        "            'product_review': review_text,\n",
        "            'user_rating': rating if rating else 0\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def extract_rating_from_text(text):\n",
        "    \"\"\"Extract rating with comprehensive patterns\"\"\"\n",
        "    rating_patterns = [\n",
        "        r'(\\d+\\.?\\d*)\\s*/\\s*5',\n",
        "        r'(\\d+\\.?\\d*)\\s*/\\s*10',\n",
        "        r'(\\d+\\.?\\d*)\\s*/\\s*100',\n",
        "        r'Overall:\\s*(\\d+\\.?\\d*)',\n",
        "        r'Score:\\s*(\\d+\\.?\\d*)',\n",
        "        r'Rating:\\s*(\\d+\\.?\\d*)',\n",
        "        r'look:\\s*(\\d+\\.?\\d*)',\n",
        "        r'smell:\\s*(\\d+\\.?\\d*)',\n",
        "        r'taste:\\s*(\\d+\\.?\\d*)',\n",
        "        r'feel:\\s*(\\d+\\.?\\d*)',\n",
        "        r'overall:\\s*(\\d+\\.?\\d*)'\n",
        "    ]\n",
        "\n",
        "    for pattern in rating_patterns:\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "        if match:\n",
        "            try:\n",
        "                raw_rating = float(match.group(1))\n",
        "\n",
        "                # Convert to 5-point scale\n",
        "                if '/5' in pattern:\n",
        "                    return min(raw_rating, 5.0)\n",
        "                elif '/10' in pattern:\n",
        "                    return min(raw_rating / 2, 5.0)\n",
        "                elif '/100' in pattern:\n",
        "                    return min(raw_rating / 20, 5.0)\n",
        "                else:\n",
        "                    # Guess based on value\n",
        "                    if raw_rating <= 5:\n",
        "                        return raw_rating\n",
        "                    elif raw_rating <= 10:\n",
        "                        return raw_rating / 2\n",
        "                    elif raw_rating <= 100:\n",
        "                        return raw_rating / 20\n",
        "\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_clean_review_text(full_text):\n",
        "    \"\"\"Extract clean review text, removing metadata\"\"\"\n",
        "\n",
        "    # Remove metadata patterns\n",
        "    text = re.sub(r'Reviewed by.*?on.*?\\d{4}', '', full_text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'Location:.*?Date:.*?\\d{4}', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\d+\\.?\\d*\\s*/\\s*\\d+', ' ', text)  # Remove ratings\n",
        "    text = re.sub(r'(look|smell|taste|feel|overall):\\s*\\d+\\.?\\d*', ' ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'Serving type:.*?Container:.*?', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Clean up whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Find the longest coherent part\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    best_part = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(sentence.strip()) > len(best_part) and has_beer_content(sentence):\n",
        "            best_part = sentence.strip()\n",
        "\n",
        "    if not best_part:\n",
        "        best_part = text\n",
        "\n",
        "    return best_part[:800]  # Reasonable length limit\n",
        "\n",
        "def has_beer_content(text):\n",
        "    \"\"\"Check if text has actual beer review content\"\"\"\n",
        "    beer_keywords = [\n",
        "        'taste', 'flavor', 'aroma', 'smell', 'appearance', 'look', 'beer', 'hop', 'hops',\n",
        "        'malt', 'bitter', 'sweet', 'drink', 'pour', 'head', 'foam', 'alcohol', 'finish',\n",
        "        'aftertaste', 'body', 'mouthfeel', 'carbonation', 'crisp', 'smooth', 'dry',\n",
        "        'light', 'dark', 'amber', 'golden', 'brown', 'wheat', 'barley', 'yeast', 'brew'\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    beer_word_count = sum(1 for keyword in beer_keywords if keyword in text_lower)\n",
        "\n",
        "    return beer_word_count >= 2\n",
        "\n",
        "def is_meaningless_text(text):\n",
        "    \"\"\"Filter out meaningless/spam text\"\"\"\n",
        "    meaningless_patterns = [\n",
        "        r'^[\\d\\s\\.\\-/]+$',  # Just numbers and symbols\n",
        "        r'^[A-Za-z\\s]{1,20}$',  # Too short and generic\n",
        "        r'test|spam|asdf|qwerty',  # Common spam\n",
        "    ]\n",
        "\n",
        "    for pattern in meaningless_patterns:\n",
        "        if re.search(pattern, text, re.IGNORECASE):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def save_reviews_to_csv(reviews):\n",
        "    if not reviews:\n",
        "        return\n",
        "\n",
        "    with open(CSV_FILENAME, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\n",
        "            'product_name', 'product_review', 'user_rating'\n",
        "        ])\n",
        "        for review in reviews:\n",
        "            writer.writerow(review)\n",
        "\n",
        "def save_beers_to_csv(beers):\n",
        "    with open(PRODUCTS_FILENAME, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=['beer_name', 'url'])\n",
        "        for beer in beers:\n",
        "            writer.writerow(beer)\n",
        "\n",
        "# MAIN EXECUTION\n",
        "init_csv_files()\n",
        "print(\"Setup complete\")\n",
        "\n",
        "# Get beer list\n",
        "print(\"Getting beer list...\")\n",
        "beer_list = extract_beer_list()\n",
        "\n",
        "if not beer_list:\n",
        "    print(\"No beers found\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Got {len(beer_list)} beers\")\n",
        "save_beers_to_csv(beer_list)\n",
        "\n",
        "# Setup driver and login\n",
        "driver = setup_chrome_driver()\n",
        "logged_in = login_to_beeradvocate(driver)\n",
        "\n",
        "# Scrape reviews - ONLY COUNT QUALITY ONES\n",
        "print(f\"\\nScraping quality reviews (target: {TARGET_QUALITY_REVIEWS})...\")\n",
        "print(f\"Minimum review length: {MIN_REVIEW_LENGTH} characters\")\n",
        "\n",
        "total_quality_reviews = 0\n",
        "\n",
        "try:\n",
        "    for i, beer in enumerate(beer_list):\n",
        "        if total_quality_reviews >= TARGET_QUALITY_REVIEWS:\n",
        "            print(f\"🎯 TARGET REACHED! Got {total_quality_reviews} quality reviews\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            remaining = TARGET_QUALITY_REVIEWS - total_quality_reviews\n",
        "            print(f\"[{i+1}/{len(beer_list)}] {beer['beer_name'][:45]}... (need {remaining} more)\")\n",
        "\n",
        "            # Get quality reviews from this beer (multiple pages)\n",
        "            quality_reviews = extract_reviews_from_beer_multipage(driver, beer)\n",
        "\n",
        "            if quality_reviews:\n",
        "                save_reviews_to_csv(quality_reviews)\n",
        "                total_quality_reviews += len(quality_reviews)\n",
        "                print(f\"   ✅ +{len(quality_reviews)} quality reviews (Total: {total_quality_reviews}/{TARGET_QUALITY_REVIEWS})\")\n",
        "            else:\n",
        "                print(\"   ❌ No quality reviews found\")\n",
        "\n",
        "            # Progress checkpoints\n",
        "            if (i + 1) % 15 == 0:\n",
        "                print(f\"\\n📊 CHECKPOINT: {total_quality_reviews}/{TARGET_QUALITY_REVIEWS} quality reviews\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n⏹️ Stopped by user\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error: {e}\")\n",
        "            continue\n",
        "\n",
        "finally:\n",
        "    driver.quit()\n",
        "\n",
        "print(f\"\\n🎉 SCRAPING COMPLETE!\")\n",
        "print(f\"Total QUALITY reviews collected: {total_quality_reviews}\")\n",
        "\n",
        "# Final analysis\n",
        "try:\n",
        "    df = pd.read_csv(CSV_FILENAME)\n",
        "    print(f\"\\n📈 FINAL ANALYSIS:\")\n",
        "    print(f\"Dataset size: {len(df)} reviews\")\n",
        "\n",
        "    if len(df) > 0:\n",
        "        print(f\"Average rating: {df['user_rating'].mean():.2f}/5.0\")\n",
        "        print(f\"Average review length: {df['product_review'].str.len().mean():.0f} characters\")\n",
        "        print(f\"Reviews with ratings: {len(df[df['user_rating'] > 0])}\")\n",
        "        print(f\"Unique beers: {df['product_name'].nunique()}\")\n",
        "\n",
        "        print(f\"\\n📝 SAMPLE REVIEW:\")\n",
        "        sample = df.iloc[0]\n",
        "        print(f\"Beer: {sample['product_name']}\")\n",
        "        print(f\"Rating: {sample['user_rating']}\")\n",
        "        print(f\"Review: {sample['product_review'][:250]}...\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not analyze final data: {e}\")\n",
        "\n",
        "print(\"\\n✅ DONE!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp5d-mTo5Vpp",
        "outputId": "dec78215-f004-41bd-c57b-7a209fc0b3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded 2875 rows\n",
            "File ready for your Task B code\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Navigate to your file - it will be in /content/drive/MyDrive/\n",
        "# Replace 'your_filename.csv' with your actual filename\n",
        "df = pd.read_csv('/content/drive/MyDrive/beeradvocate_reviews.csv')\n",
        "print(f\"Loaded {len(df)} rows\")\n",
        "\n",
        "# Save it locally in Colab for your existing code to use\n",
        "df.to_csv('beeradvocate_reviews.csv', index=False)\n",
        "print(\"File ready for your Task B code\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6H6Vv8iDbZI"
      },
      "source": [
        "Task B: Assume that a customer, who will be using this recommender system, has specified 3 attributes\n",
        "in a product. E.g., one website describes multiple attributes of beer (but you should choose attributes\n",
        "from the actual data like you did for the first assignment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqsBZj4KDiBW"
      },
      "source": [
        "**Chosen Attributes**: Chocolate, Dark, Coffee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r98mIk7ofzVe",
        "outputId": "b3d629fb-3d75-44ba-c2a1-d10915a44cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍺 Task B: Truly Data-Driven Beer Recommendation System\n",
            "======================================================================\n",
            "Loading beer review data...\n",
            "✓ Loaded 3034 reviews for 249 unique beers\n",
            "Preprocessing review text...\n",
            "Removed 4 short reviews\n",
            "✓ 3030 reviews remaining for analysis\n",
            "\n",
            "======================================================================\n",
            "STEP 2: DATA-DRIVEN ATTRIBUTE DISCOVERY\n",
            "======================================================================\n",
            "Analyzing review text to discover descriptive attributes...\n",
            "Extracting all unique words from reviews...\n",
            "Found 2687 potential descriptive words\n",
            "Filtering for beer-related descriptive terms...\n",
            "Identified 735 beer-related descriptive terms\n",
            "\n",
            "Top 20 discovered beer attributes from review data:\n",
            "   1. chocolate       (1209 mentions)\n",
            "   2. dark            (1137 mentions)\n",
            "   3. coffee          ( 909 mentions)\n",
            "   4. sweet           ( 872 mentions)\n",
            "   5. bourbon         ( 864 mentions)\n",
            "   6. vanilla         ( 849 mentions)\n",
            "   7. nose            ( 795 mentions)\n",
            "   8. notes           ( 771 mentions)\n",
            "   9. light           ( 755 mentions)\n",
            "  10. black           ( 733 mentions)\n",
            "  11. pours           ( 714 mentions)\n",
            "  12. aroma           ( 649 mentions)\n",
            "  13. brown           ( 643 mentions)\n",
            "  14. barrel          ( 638 mentions)\n",
            "  15. medium          ( 593 mentions)\n",
            "  16. lacing          ( 584 mentions)\n",
            "  17. glass           ( 565 mentions)\n",
            "  18. body            ( 552 mentions)\n",
            "  19. carbonation     ( 548 mentions)\n",
            "  20. finish          ( 547 mentions)\n",
            "\n",
            "======================================================================\n",
            "STEP 3: LIFT ANALYSIS FOR ATTRIBUTE CO-OCCURRENCE\n",
            "======================================================================\n",
            "Creating attribute co-occurrence matrix...\n",
            "Analyzing lift for 30 frequent attributes...\n",
            "\n",
            "Top 15 attribute pairs with highest lift ratios:\n",
            "Rank Attribute Pair                 Lift Ratio   Co-occurrences \n",
            "-----------------------------------------------------------------\n",
            "  1. white & orange                    3.603            158\n",
            "  2. dark & brown                      3.044            350\n",
            "  3. chocolate & black                 2.976            458\n",
            "  4. carbonation & medium              2.912            246\n",
            "  5. brown & caramel                   2.868            136\n",
            "  6. coffee & black                    2.791            312\n",
            "  7. chocolate & coffee                2.769            348\n",
            "  8. body & medium                     2.745            217\n",
            "  9. bourbon & vanilla                 2.687            264\n",
            " 10. bourbon & brown                   2.658            227\n",
            " 11. vanilla & black                   2.643            318\n",
            " 12. bourbon & black                   2.639            295\n",
            " 13. chocolate & dark                  2.601            440\n",
            " 14. chocolate & vanilla               2.566            347\n",
            " 15. bourbon & dark                    2.547            313\n",
            "\n",
            "======================================================================\n",
            "STEP 4: DATA-DRIVEN CUSTOMER ATTRIBUTE SELECTION\n",
            "======================================================================\n",
            "Selection Strategy:\n",
            "1. Use frequency analysis to find most common descriptors\n",
            "2. Use lift analysis to find attributes that co-occur\n",
            "3. Select 3 attributes that are both frequent AND have high co-occurrence\n",
            "\n",
            "Attributes that are both frequent AND co-occur well:\n",
            "   1. chocolate       (1209 mentions, in 2 top lift pairs)\n",
            "   2. dark            (1137 mentions, in 1 top lift pairs)\n",
            "   3. coffee          ( 909 mentions, in 2 top lift pairs)\n",
            "   4. bourbon         ( 864 mentions, in 2 top lift pairs)\n",
            "   5. vanilla         ( 849 mentions, in 1 top lift pairs)\n",
            "   6. black           ( 733 mentions, in 2 top lift pairs)\n",
            "   7. brown           ( 643 mentions, in 3 top lift pairs)\n",
            "   8. medium          ( 593 mentions, in 2 top lift pairs)\n",
            "\n",
            "🎯 SELECTED CUSTOMER ATTRIBUTES (Data-Driven):\n",
            "  1. chocolate       (1209 mentions) - ✓ High frequency + High co-occurrence\n",
            "  2. dark            (1137 mentions) - ✓ High frequency + High co-occurrence\n",
            "  3. coffee          ( 909 mentions) - ✓ High frequency + High co-occurrence\n",
            "\n",
            "Customer Simulation:\n",
            "A customer says: 'I want a beer that is chocolate, dark, and coffee'\n",
            "\n",
            "======================================================================\n",
            "STEP 5: SENTIMENT ANALYSIS\n",
            "======================================================================\n",
            "Calculating sentiment scores for all reviews...\n",
            "Sentiment Distribution:\n",
            "  Positive (>0.1):   2064 reviews (68.1%)\n",
            "  Neutral (-0.1-0.1):   901 reviews (29.7%)\n",
            "  Negative (<-0.1):     65 reviews (2.1%)\n",
            "\n",
            "======================================================================\n",
            "STEP 6: BEER PROFILING & RECOMMENDATION MODEL\n",
            "======================================================================\n",
            "Creating comprehensive beer profiles...\n",
            "✓ Created profiles for 243 beers with 3+ reviews each\n",
            "Building TF-IDF model for semantic similarity...\n",
            "✓ Calculated similarity scores for 243 beers\n",
            "\n",
            "======================================================================\n",
            "STEP 7: GENERATING RECOMMENDATIONS\n",
            "======================================================================\n",
            "✓ Recommendations generated and ranked\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULTS: BEER RECOMMENDATIONS\n",
            "================================================================================\n",
            "Customer Request: 'I want a beer that is chocolate, dark, coffee'\n",
            "These attributes were selected through data-driven analysis of 3030 reviews\n",
            "\n",
            "Recommendation Methodology:\n",
            "1. Discovered 735 attributes directly from review text\n",
            "2. Selected top 3 attributes using frequency + co-occurrence analysis\n",
            "3. Used TF-IDF + cosine similarity to match customer preferences\n",
            "4. Combined similarity (40%) + rating (30%) + sentiment (20%) + popularity (10%)\n",
            "\n",
            "================================================================================\n",
            "TOP 3 FINAL RECOMMENDATIONS + 20 CONTENDERS\n",
            "================================================================================\n",
            "Rank Beer Name                                Similarity Rating   Sentiment  Reviews  Final Score\n",
            "-----------------------------------------------------------------------------------------------\n",
            "🏆1   Triple Shot                              0.621      4.50     0.169      6        0.655\n",
            "🏆2   KBS                                      0.489      4.65     0.328      10       0.631\n",
            "🏆3   Parabajava                               0.417      4.55     0.196      10       0.583\n",
            " 4.  Speedway Stout - Bourbon Barrel-Aged     0.463      4.32     0.074      15       0.579\n",
            " 5.  Breakfast Stout                          0.471      3.90     0.341      5        0.574\n",
            " 6.  Kaggen! Stormaktsporter                  0.393      4.54     0.148      18       0.574\n",
            " 7.  CBS (Canadian Breakfast Stout)           0.356      4.52     0.247      13       0.565\n",
            " 8.  Coffee Cinnamon B-Bomb                   0.337      4.54     0.218      20       0.559\n",
            " 9.  Parabola                                 0.338      4.45     0.217      19       0.554\n",
            "10.  Speedway Stout - Vietnamese Coffee - Bo  0.358      4.40     0.211      10       0.552\n",
            "11.  Barrel-Aged Sump Coffee Stout            0.376      4.42     0.172      6        0.552\n",
            "12.  All That Is And All That Ever Will Be    0.352      4.47     0.101      15       0.546\n",
            "13.  Caffè Americano                          0.349      4.32     0.148      19       0.543\n",
            "14.  Red Eye November                         0.305      4.64     0.139      16       0.543\n",
            "15.  Abraxas - Coffee                         0.321      4.51     0.126      17       0.541\n",
            "16.  Affogato                                 0.329      4.40     0.152      13       0.537\n",
            "17.  Double Shot                              0.384      4.25     0.127      4        0.537\n",
            "18.  Mornin' Delight                          0.280      4.58     0.265      10       0.537\n",
            "19.  Bomb!                                    0.318      4.36     0.160      21       0.535\n",
            "20.  Bourbon Barrel Champion Ground           0.318      4.43     0.115      19       0.535\n",
            "21.  Moment Of Clarity                        0.343      4.40     0.141      5        0.533\n",
            "22.  Mocha Wednesday                          0.331      4.30     0.120      20       0.533\n",
            "23.  Speedway Stout - Vietnamese Coffee - Ry  0.264      4.60     0.182      12       0.526\n",
            "\n",
            "================================================================================\n",
            "DETAILED ANALYSIS OF TOP 3 RECOMMENDATIONS\n",
            "================================================================================\n",
            "\n",
            "🏆 RECOMMENDATION #1: Triple Shot\n",
            "   Why this beer was selected:\n",
            "   • Content Similarity:  0.621 (how well it matches 'chocolate, dark, coffee')\n",
            "   • Average Rating:      4.50/5.0 (overall quality)\n",
            "   • Sentiment Score:     0.169 (reviewer satisfaction)\n",
            "   • Review Count:        6 (reliability)\n",
            "   • Final Composite Score: 0.655\n",
            "\n",
            "🏆 RECOMMENDATION #2: KBS\n",
            "   Why this beer was selected:\n",
            "   • Content Similarity:  0.489 (how well it matches 'chocolate, dark, coffee')\n",
            "   • Average Rating:      4.65/5.0 (overall quality)\n",
            "   • Sentiment Score:     0.328 (reviewer satisfaction)\n",
            "   • Review Count:        10 (reliability)\n",
            "   • Final Composite Score: 0.631\n",
            "\n",
            "🏆 RECOMMENDATION #3: Parabajava\n",
            "   Why this beer was selected:\n",
            "   • Content Similarity:  0.417 (how well it matches 'chocolate, dark, coffee')\n",
            "   • Average Rating:      4.55/5.0 (overall quality)\n",
            "   • Sentiment Score:     0.196 (reviewer satisfaction)\n",
            "   • Review Count:        10 (reliability)\n",
            "   • Final Composite Score: 0.583\n",
            "\n",
            "================================================================================\n",
            "SAMPLE REVIEWS FOR TOP RECOMMENDATION\n",
            "================================================================================\n",
            "Beer: Triple Shot\n",
            "\n",
            "Sample Review 1 (Rating: 4.5/5, Sentiment: 0.205):\n",
            "'. I'm pleasantly surprised by how sweet this isn't, just a hint of rich semi-sweet chocolate at finish. The nutty, roasty base makes for a wonderful coffee showcase throughout. Report...'\n",
            "\n",
            "Sample Review 2 (Rating: 4.5/5, Sentiment: 0.025):\n",
            "'. | | Pours dark brown. Aroma dominanted by coffee & chocolate. Taste is coffee, chocolate and a tinge of caramel. Medium mouthfeel. Very tasty stout. Report...'\n",
            "\n",
            "Sample Review 3 (Rating: 4.5/5, Sentiment: -0.062):\n",
            "'! Smells like cold brew coffee and chocolate. Medium thick mouthfeel. Tastes exactly as it smells. Tons of coffee on the aftertaste. It’s fantastic. Report...'\n",
            "\n",
            "================================================================================\n",
            "DATA-DRIVEN ANALYSIS SUMMARY\n",
            "================================================================================\n",
            "Dataset Analysis:\n",
            "  • Total reviews processed: 3030\n",
            "  • Unique beers analyzed: 249\n",
            "  • Descriptive attributes discovered: 735\n",
            "  • Attribute pairs analyzed for co-occurrence: 431\n",
            "\n",
            "Attribute Selection Process:\n",
            "  • Discovered attributes from actual review text (not predefined)\n",
            "  • Used frequency analysis to identify common descriptors\n",
            "  • Applied lift analysis to find attributes that co-occur\n",
            "  • Selected final 3 attributes: ['chocolate', 'dark', 'coffee']\n",
            "\n",
            "Recommendation Quality:\n",
            "  • Beers evaluated: 243\n",
            "  • Average similarity to customer preferences: 0.093\n",
            "  • Top recommendation similarity: 0.621\n",
            "\n",
            "✅ Task B Complete: Data-driven beer recommendation system ready!\n",
            "Next: Implement Task C (word embeddings comparison)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"🍺 Task B: Truly Data-Driven Beer Recommendation System\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ================================\n",
        "# STEP 1: DATA LOADING & PREPROCESSING\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    print(\"Loading beer review data...\")\n",
        "    df = pd.read_csv('beeradvocate_reviews.csv')\n",
        "    print(f\"✓ Loaded {len(df)} reviews for {df['product_name'].nunique()} unique beers\")\n",
        "\n",
        "    # Basic validation\n",
        "    if len(df) == 0:\n",
        "        print(\"❌ Error: Dataset is empty!\")\n",
        "        exit(1)\n",
        "\n",
        "    required_columns = ['product_name', 'product_review', 'user_rating']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"❌ Error: Missing required columns: {missing_columns}\")\n",
        "        exit(1)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ Error: 'beeradvocate_reviews.csv' not found!\")\n",
        "    print(\"Please run the scraper first to generate the data.\")\n",
        "    exit(1)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess review text for analysis\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text).lower()\n",
        "    # Remove special characters but keep spaces\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Clean the data\n",
        "print(\"Preprocessing review text...\")\n",
        "df['clean_review'] = df['product_review'].apply(clean_text)\n",
        "df['user_rating'] = pd.to_numeric(df['user_rating'], errors='coerce').fillna(0)\n",
        "\n",
        "# Remove very short reviews\n",
        "original_count = len(df)\n",
        "df = df[df['clean_review'].str.len() > 30].copy()\n",
        "print(f\"Removed {original_count - len(df)} short reviews\")\n",
        "print(f\"✓ {len(df)} reviews remaining for analysis\")\n",
        "\n",
        "# ================================\n",
        "# STEP 2: TRULY DATA-DRIVEN ATTRIBUTE DISCOVERY\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"STEP 2: DATA-DRIVEN ATTRIBUTE DISCOVERY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "def extract_meaningful_descriptors(reviews, min_length=3, max_length=12):\n",
        "    \"\"\"Extract meaningful descriptive words from all reviews - NO PREDEFINED LISTS\"\"\"\n",
        "\n",
        "    print(\"Extracting all unique words from reviews...\")\n",
        "\n",
        "    # Get all words from all reviews\n",
        "    all_words = []\n",
        "    for review in reviews:\n",
        "        if isinstance(review, str):\n",
        "            words = review.split()\n",
        "            # Filter for reasonable word lengths\n",
        "            meaningful_words = [w for w in words if min_length <= len(w) <= max_length]\n",
        "            all_words.extend(meaningful_words)\n",
        "\n",
        "    # Count frequency of all words\n",
        "    word_freq = Counter(all_words)\n",
        "\n",
        "    # Remove common stop words and non-descriptive words\n",
        "    stop_words = {\n",
        "    # Original stop words\n",
        "    'the', 'and', 'but', 'for', 'are', 'was', 'were', 'been', 'have', 'has', 'had',\n",
        "    'this', 'that', 'with', 'from', 'they', 'them', 'their', 'said', 'each', 'which',\n",
        "    'what', 'where', 'when', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
        "    'most', 'other', 'some', 'such', 'only', 'own', 'same', 'than', 'too', 'very',\n",
        "    'can', 'will', 'just', 'should', 'now', 'also', 'back', 'after', 'first', 'well',\n",
        "    'way', 'even', 'new', 'want', 'because', 'good', 'great', 'bad', 'nice', 'like',\n",
        "    'really', 'much', 'many', 'little', 'long', 'time', 'year', 'get', 'make', 'take',\n",
        "    'come', 'could', 'would', 'there', 'here', 'then', 'than', 'these', 'those',\n",
        "\n",
        "    # Beer review generic terms (new additions)\n",
        "    'beer', 'taste', 'your', 'review', 'smell', 'look', 'overall', 'feel', 'out',\n",
        "    'head', 'don', 'plus', 'need', 'impression', 'drop', 'attributes', 'thoughts',\n",
        "    'you', 'not', 'one', 'may', 'into', 'its', 'about', 'see', 'know', 'think',\n",
        "    'got', 'got', 'went', 'went', 'use', 'used', 'find', 'found', 'come', 'came'\n",
        "}\n",
        "\n",
        "    # Filter out stop words and get potential descriptors\n",
        "    potential_descriptors = {word: count for word, count in word_freq.items()\n",
        "                           if word not in stop_words and count >= 5}  # Minimum 5 occurrences\n",
        "\n",
        "    print(f\"Found {len(potential_descriptors)} potential descriptive words\")\n",
        "\n",
        "    # Further filter for words that are likely to be beer descriptors\n",
        "    # Look for adjectives and beer-related terms by checking context\n",
        "    beer_descriptors = {}\n",
        "\n",
        "    print(\"Filtering for beer-related descriptive terms...\")\n",
        "\n",
        "    # Keywords that indicate beer-related context\n",
        "    beer_context_words = {\n",
        "        'beer', 'ale', 'lager', 'stout', 'porter', 'ipa', 'brew', 'brewing', 'brewed',\n",
        "        'taste', 'tastes', 'tasting', 'flavor', 'flavors', 'flavored', 'aroma', 'smell',\n",
        "        'appearance', 'look', 'looks', 'color', 'head', 'foam', 'carbonation',\n",
        "        'mouthfeel', 'body', 'finish', 'aftertaste', 'alcohol', 'abv', 'drink', 'drinking'\n",
        "    }\n",
        "\n",
        "    # For each potential descriptor, check if it commonly appears near beer context words\n",
        "    for word, count in potential_descriptors.items():\n",
        "        if count < 10:  # Need reasonable frequency\n",
        "            continue\n",
        "\n",
        "        # Check if this word appears in beer-related contexts\n",
        "        beer_context_count = 0\n",
        "        total_appearances = 0\n",
        "\n",
        "        for review in reviews[:1000]:  # Sample to speed up processing\n",
        "            if isinstance(review, str) and word in review:\n",
        "                total_appearances += 1\n",
        "                review_words = review.split()\n",
        "\n",
        "                # Check if any beer context words appear near this word\n",
        "                if word in review_words:\n",
        "                    word_positions = [i for i, w in enumerate(review_words) if w == word]\n",
        "\n",
        "                    for pos in word_positions:\n",
        "                        # Check words within 5 positions before and after\n",
        "                        context_start = max(0, pos - 5)\n",
        "                        context_end = min(len(review_words), pos + 6)\n",
        "                        context_words = review_words[context_start:context_end]\n",
        "\n",
        "                        if any(ctx_word in beer_context_words for ctx_word in context_words):\n",
        "                            beer_context_count += 1\n",
        "                            break\n",
        "\n",
        "        # If word appears in beer context frequently enough, it's likely a beer descriptor\n",
        "        if total_appearances > 0 and (beer_context_count / total_appearances) > 0.3:\n",
        "            beer_descriptors[word] = count\n",
        "\n",
        "    print(f\"Identified {len(beer_descriptors)} beer-related descriptive terms\")\n",
        "\n",
        "    return beer_descriptors\n",
        "\n",
        "# Extract descriptors from actual review data\n",
        "print(\"Analyzing review text to discover descriptive attributes...\")\n",
        "beer_descriptors = extract_meaningful_descriptors(df['clean_review'])\n",
        "\n",
        "# Get top attributes by frequency\n",
        "top_discovered_attributes = sorted(beer_descriptors.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"\\nTop 20 discovered beer attributes from review data:\")\n",
        "for i, (attr, count) in enumerate(top_discovered_attributes[:20], 1):\n",
        "    print(f\"  {i:2d}. {attr:15} ({count:4d} mentions)\")\n",
        "\n",
        "# ================================\n",
        "# STEP 3: LIFT ANALYSIS FOR CO-OCCURRENCE\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"STEP 3: LIFT ANALYSIS FOR ATTRIBUTE CO-OCCURRENCE\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "def find_attributes_in_review(review_text, attribute_list):\n",
        "    \"\"\"Find which attributes appear in a given review\"\"\"\n",
        "    if not isinstance(review_text, str):\n",
        "        return []\n",
        "\n",
        "    words_in_review = set(review_text.split())\n",
        "    found_attributes = [attr for attr in attribute_list if attr in words_in_review]\n",
        "    return found_attributes\n",
        "\n",
        "# Create attribute occurrence matrix\n",
        "print(\"Creating attribute co-occurrence matrix...\")\n",
        "\n",
        "# Use top 30 attributes for lift analysis\n",
        "top_30_attrs = [attr for attr, count in top_discovered_attributes[:30]]\n",
        "\n",
        "# Find attributes in each review\n",
        "df['found_attributes'] = df['clean_review'].apply(\n",
        "    lambda x: find_attributes_in_review(x, top_30_attrs)\n",
        ")\n",
        "\n",
        "def calculate_lift_from_data(df, min_support=10):\n",
        "    \"\"\"Calculate lift ratios for discovered attributes\"\"\"\n",
        "\n",
        "    # Count individual attribute frequencies\n",
        "    attr_counts = Counter()\n",
        "    for attr_list in df['found_attributes']:\n",
        "        if isinstance(attr_list, list):\n",
        "            attr_counts.update(attr_list)\n",
        "\n",
        "    # Filter for frequent attributes\n",
        "    frequent_attrs = [attr for attr, count in attr_counts.items() if count >= min_support]\n",
        "    total_reviews = len(df)\n",
        "\n",
        "    print(f\"Analyzing lift for {len(frequent_attrs)} frequent attributes...\")\n",
        "\n",
        "    # Calculate lift for all pairs\n",
        "    lift_results = []\n",
        "\n",
        "    for i, attr1 in enumerate(frequent_attrs):\n",
        "        for j, attr2 in enumerate(frequent_attrs):\n",
        "            if i >= j:  # Avoid duplicates and self-pairs\n",
        "                continue\n",
        "\n",
        "            # Count co-occurrences\n",
        "            cooccur_count = 0\n",
        "            for attr_list in df['found_attributes']:\n",
        "                if isinstance(attr_list, list) and attr1 in attr_list and attr2 in attr_list:\n",
        "                    cooccur_count += 1\n",
        "\n",
        "            if cooccur_count >= 3:  # Minimum co-occurrences\n",
        "                # Calculate probabilities\n",
        "                prob_a = attr_counts[attr1] / total_reviews\n",
        "                prob_b = attr_counts[attr2] / total_reviews\n",
        "                prob_ab = cooccur_count / total_reviews\n",
        "\n",
        "                # Calculate lift\n",
        "                lift_ratio = prob_ab / (prob_a * prob_b)\n",
        "\n",
        "                lift_results.append({\n",
        "                    'attr1': attr1,\n",
        "                    'attr2': attr2,\n",
        "                    'cooccur_count': cooccur_count,\n",
        "                    'lift_ratio': lift_ratio\n",
        "                })\n",
        "\n",
        "    # Sort by lift ratio\n",
        "    lift_results.sort(key=lambda x: x['lift_ratio'], reverse=True)\n",
        "\n",
        "    return lift_results, frequent_attrs\n",
        "\n",
        "# Calculate lift analysis\n",
        "lift_results, frequent_attrs = calculate_lift_from_data(df, min_support=15)\n",
        "\n",
        "print(f\"\\nTop 15 attribute pairs with highest lift ratios:\")\n",
        "print(f\"{'Rank':<4} {'Attribute Pair':<30} {'Lift Ratio':<12} {'Co-occurrences':<15}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for i, result in enumerate(lift_results[:15], 1):\n",
        "    pair_name = f\"{result['attr1']} & {result['attr2']}\"\n",
        "    print(f\"{i:3d}. {pair_name:<30} {result['lift_ratio']:>8.3f}     {result['cooccur_count']:>10d}\")\n",
        "\n",
        "# ================================\n",
        "# STEP 4: DATA-DRIVEN ATTRIBUTE SELECTION\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"STEP 4: DATA-DRIVEN CUSTOMER ATTRIBUTE SELECTION\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"Selection Strategy:\")\n",
        "print(\"1. Use frequency analysis to find most common descriptors\")\n",
        "print(\"2. Use lift analysis to find attributes that co-occur\")\n",
        "print(\"3. Select 3 attributes that are both frequent AND have high co-occurrence\")\n",
        "\n",
        "# Strategy: Select attributes that appear in top frequency AND top lift pairs\n",
        "high_frequency_attrs = set([attr for attr, count in top_discovered_attributes[:15]])\n",
        "high_lift_attrs = set()\n",
        "\n",
        "# Get attributes from top lift pairs\n",
        "for result in lift_results[:10]:\n",
        "    high_lift_attrs.add(result['attr1'])\n",
        "    high_lift_attrs.add(result['attr2'])\n",
        "\n",
        "# Find intersection - attributes that are both frequent and co-occur well\n",
        "candidate_attrs = high_frequency_attrs.intersection(high_lift_attrs)\n",
        "\n",
        "print(f\"\\nAttributes that are both frequent AND co-occur well:\")\n",
        "selected_candidates = []\n",
        "for attr in top_discovered_attributes:\n",
        "    if attr[0] in candidate_attrs:\n",
        "        selected_candidates.append(attr)\n",
        "\n",
        "for i, (attr, count) in enumerate(selected_candidates[:10], 1):\n",
        "    lift_appearances = sum(1 for result in lift_results[:10]\n",
        "                          if attr in [result['attr1'], result['attr2']])\n",
        "    print(f\"  {i:2d}. {attr:15} ({count:4d} mentions, in {lift_appearances} top lift pairs)\")\n",
        "\n",
        "# Select final 3 attributes\n",
        "if len(selected_candidates) >= 3:\n",
        "    selected_attributes = [attr for attr, count in selected_candidates[:3]]\n",
        "else:\n",
        "    # Fallback: use top frequency attributes\n",
        "    selected_attributes = [attr for attr, count in top_discovered_attributes[:3]]\n",
        "\n",
        "print(f\"\\n🎯 SELECTED CUSTOMER ATTRIBUTES (Data-Driven):\")\n",
        "for i, attr in enumerate(selected_attributes, 1):\n",
        "    attr_count = dict(top_discovered_attributes).get(attr, 0)\n",
        "    in_lift = any(attr in [r['attr1'], r['attr2']] for r in lift_results[:10])\n",
        "    status = \"✓ High frequency + High co-occurrence\" if in_lift else \"✓ High frequency\"\n",
        "    print(f\"  {i}. {attr:15} ({attr_count:4d} mentions) - {status}\")\n",
        "\n",
        "print(f\"\\nCustomer Simulation:\")\n",
        "print(f\"A customer says: 'I want a beer that is {selected_attributes[0]}, {selected_attributes[1]}, and {selected_attributes[2]}'\")\n",
        "\n",
        "# ================================\n",
        "# STEP 5: SENTIMENT ANALYSIS\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"STEP 5: SENTIMENT ANALYSIS\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "def get_sentiment_score(text):\n",
        "    \"\"\"Calculate sentiment polarity using TextBlob\"\"\"\n",
        "    try:\n",
        "        blob = TextBlob(str(text))\n",
        "        return blob.sentiment.polarity  # Returns -1 to 1\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "print(\"Calculating sentiment scores for all reviews...\")\n",
        "df['sentiment'] = df['product_review'].apply(get_sentiment_score)\n",
        "\n",
        "# Display sentiment distribution\n",
        "positive_count = sum(df['sentiment'] > 0.1)\n",
        "neutral_count = sum((df['sentiment'] >= -0.1) & (df['sentiment'] <= 0.1))\n",
        "negative_count = sum(df['sentiment'] < -0.1)\n",
        "\n",
        "print(f\"Sentiment Distribution:\")\n",
        "print(f\"  Positive (>0.1):  {positive_count:5d} reviews ({positive_count/len(df)*100:.1f}%)\")\n",
        "print(f\"  Neutral (-0.1-0.1): {neutral_count:5d} reviews ({neutral_count/len(df)*100:.1f}%)\")\n",
        "print(f\"  Negative (<-0.1):  {negative_count:5d} reviews ({negative_count/len(df)*100:.1f}%)\")\n",
        "\n",
        "# ================================\n",
        "# STEP 6: BEER PROFILING & TF-IDF MODELING\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"STEP 6: BEER PROFILING & RECOMMENDATION MODEL\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"Creating comprehensive beer profiles...\")\n",
        "\n",
        "# Aggregate reviews by beer\n",
        "beer_profiles = df.groupby('product_name').agg({\n",
        "    'clean_review': lambda x: ' '.join(x),\n",
        "    'user_rating': 'mean',\n",
        "    'sentiment': 'mean',\n",
        "    'product_review': 'count'\n",
        "}).rename(columns={'product_review': 'review_count'})\n",
        "\n",
        "# Filter beers with sufficient reviews for reliability\n",
        "min_reviews = max(2, len(df) // (df['product_name'].nunique() * 4))\n",
        "reliable_beers = beer_profiles[beer_profiles['review_count'] >= min_reviews]\n",
        "\n",
        "print(f\"✓ Created profiles for {len(reliable_beers)} beers with {min_reviews}+ reviews each\")\n",
        "\n",
        "# Create customer preference profile\n",
        "customer_profile = ' '.join(selected_attributes * 5)  # Repeat for emphasis\n",
        "\n",
        "# Combine customer profile with beer profiles\n",
        "all_profiles = [customer_profile] + reliable_beers['clean_review'].tolist()\n",
        "\n",
        "print(\"Building TF-IDF model for semantic similarity...\")\n",
        "\n",
        "# Create TF-IDF vectors\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=min(1000, len(' '.join(all_profiles).split())),\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=1,\n",
        "    max_df=0.95\n",
        ")\n",
        "\n",
        "tfidf_matrix = tfidf.fit_transform(all_profiles)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "customer_vector = tfidf_matrix[0]\n",
        "beer_vectors = tfidf_matrix[1:]\n",
        "similarities = cosine_similarity(customer_vector, beer_vectors).flatten()\n",
        "\n",
        "print(f\"✓ Calculated similarity scores for {len(similarities)} beers\")\n",
        "\n",
        "# ================================\n",
        "# STEP 7: RECOMMENDATION GENERATION\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"STEP 7: GENERATING RECOMMENDATIONS\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Create recommendations dataframe\n",
        "recommendations = pd.DataFrame({\n",
        "    'beer_name': reliable_beers.index,\n",
        "    'cosine_similarity': similarities,\n",
        "    'avg_rating': reliable_beers['user_rating'],\n",
        "    'avg_sentiment': reliable_beers['sentiment'],\n",
        "    'review_count': reliable_beers['review_count']\n",
        "})\n",
        "\n",
        "# Handle any NaN values\n",
        "recommendations = recommendations.fillna(0)\n",
        "\n",
        "# Calculate composite score (weighted combination)\n",
        "recommendations['composite_score'] = (\n",
        "    0.4 * recommendations['cosine_similarity'] +                    # 40% Content similarity\n",
        "    0.3 * (recommendations['avg_rating'] / 5.0) +                  # 30% Average rating\n",
        "    0.2 * ((recommendations['avg_sentiment'] + 1) / 2) +           # 20% Sentiment\n",
        "    0.1 * np.log(recommendations['review_count'] + 1) / 10         # 10% Popularity\n",
        ")\n",
        "\n",
        "# Sort by composite score\n",
        "recommendations = recommendations.sort_values('composite_score', ascending=False)\n",
        "\n",
        "print(\"✓ Recommendations generated and ranked\")\n",
        "\n",
        "# ================================\n",
        "# STEP 8: RESULTS DISPLAY\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"FINAL RESULTS: BEER RECOMMENDATIONS\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f\"Customer Request: 'I want a beer that is {', '.join(selected_attributes)}'\")\n",
        "print(f\"These attributes were selected through data-driven analysis of {len(df)} reviews\")\n",
        "\n",
        "print(f\"\\nRecommendation Methodology:\")\n",
        "print(f\"1. Discovered {len(beer_descriptors)} attributes directly from review text\")\n",
        "print(f\"2. Selected top 3 attributes using frequency + co-occurrence analysis\")\n",
        "print(f\"3. Used TF-IDF + cosine similarity to match customer preferences\")\n",
        "print(f\"4. Combined similarity (40%) + rating (30%) + sentiment (20%) + popularity (10%)\")\n",
        "\n",
        "# Display Top 3 + 20 Contenders as required\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"TOP 3 FINAL RECOMMENDATIONS + 20 CONTENDERS\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "top_23 = recommendations.head(23)\n",
        "\n",
        "print(f\"{'Rank':<4} {'Beer Name':<40} {'Similarity':<10} {'Rating':<8} {'Sentiment':<10} {'Reviews':<8} {'Final Score':<10}\")\n",
        "print(\"-\" * 95)\n",
        "\n",
        "for i, (_, row) in enumerate(top_23.iterrows(), 1):\n",
        "    if i <= 3:\n",
        "        rank_marker = f\"🏆{i}\"\n",
        "    else:\n",
        "        rank_marker = f\"{i:2d}.\"\n",
        "\n",
        "    print(f\"{rank_marker:<4} {row['beer_name'][:39]:<40} {row['cosine_similarity']:.3f}      {row['avg_rating']:.2f}     {row['avg_sentiment']:.3f}      {int(row['review_count']):<8} {row['composite_score']:.3f}\")\n",
        "\n",
        "# Detailed breakdown of top 3\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"DETAILED ANALYSIS OF TOP 3 RECOMMENDATIONS\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "for i in range(min(3, len(top_23))):\n",
        "    beer = top_23.iloc[i]\n",
        "    print(f\"\\n🏆 RECOMMENDATION #{i+1}: {beer['beer_name']}\")\n",
        "    print(f\"   Why this beer was selected:\")\n",
        "    print(f\"   • Content Similarity:  {beer['cosine_similarity']:.3f} (how well it matches '{', '.join(selected_attributes)}')\")\n",
        "    print(f\"   • Average Rating:      {beer['avg_rating']:.2f}/5.0 (overall quality)\")\n",
        "    print(f\"   • Sentiment Score:     {beer['avg_sentiment']:.3f} (reviewer satisfaction)\")\n",
        "    print(f\"   • Review Count:        {int(beer['review_count'])} (reliability)\")\n",
        "    print(f\"   • Final Composite Score: {beer['composite_score']:.3f}\")\n",
        "\n",
        "# Show sample reviews for top recommendation\n",
        "if len(top_23) > 0:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"SAMPLE REVIEWS FOR TOP RECOMMENDATION\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    top_beer_name = top_23.iloc[0]['beer_name']\n",
        "    sample_reviews = df[df['product_name'] == top_beer_name].head(3)\n",
        "\n",
        "    print(f\"Beer: {top_beer_name}\")\n",
        "\n",
        "    if len(sample_reviews) > 0:\n",
        "        for i, (_, review) in enumerate(sample_reviews.iterrows(), 1):\n",
        "            print(f\"\\nSample Review {i} (Rating: {review['user_rating']}/5, Sentiment: {review['sentiment']:.3f}):\")\n",
        "            print(f\"'{review['product_review'][:200]}...'\")\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"DATA-DRIVEN ANALYSIS SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f\"Dataset Analysis:\")\n",
        "print(f\"  • Total reviews processed: {len(df)}\")\n",
        "print(f\"  • Unique beers analyzed: {df['product_name'].nunique()}\")\n",
        "print(f\"  • Descriptive attributes discovered: {len(beer_descriptors)}\")\n",
        "print(f\"  • Attribute pairs analyzed for co-occurrence: {len(lift_results)}\")\n",
        "\n",
        "print(f\"\\nAttribute Selection Process:\")\n",
        "print(f\"  • Discovered attributes from actual review text (not predefined)\")\n",
        "print(f\"  • Used frequency analysis to identify common descriptors\")\n",
        "print(f\"  • Applied lift analysis to find attributes that co-occur\")\n",
        "print(f\"  • Selected final 3 attributes: {selected_attributes}\")\n",
        "\n",
        "print(f\"\\nRecommendation Quality:\")\n",
        "print(f\"  • Beers evaluated: {len(recommendations)}\")\n",
        "print(f\"  • Average similarity to customer preferences: {recommendations['cosine_similarity'].mean():.3f}\")\n",
        "print(f\"  • Top recommendation similarity: {recommendations.iloc[0]['cosine_similarity']:.3f}\")\n",
        "\n",
        "print(f\"\\n✅ Task B Complete: Data-driven beer recommendation system ready!\")\n",
        "print(f\"Next: Implement Task C (word embeddings comparison)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4K8CGEQD9To"
      },
      "source": [
        "For **Task B**, we scraped and preprocessed over 3K BeerAdvocate reviews, discarding no-text and very short entries. Reviews were aggregated by beer into a single document per product, and we computed each beer’s average rating, average sentiment, and review count.\n",
        "\n",
        "We used a data-driven attribute discovery step (frequency + co-occurrence lift), the three customer attributes selected from the corpus were: **chocolate**, **dark**, and **coffee**.\n",
        "\n",
        "We then applied the bag of words model and constructed a TF-IDF matrix on the beer review documents, represented the user's attributes (chocolate, dark, coffee) within the same TF-IDF space, and lastly computed a cosine similarity between the query vector and each beer vector.\n",
        "\n",
        "Additionally, we built out a composite score- which is a weighted combination of the different signals. We combined cosine similarity with average rating, sentiment, and review count into the composite score. This ensured that our recommendations were not just the beers that mentioned the right/named attributes, but also those that were well-reviewed, positively described, and overall widely sampled.\n",
        "\n",
        "The composite score was calculated as:\n",
        "$$\n",
        "\\text{Score} = 0.40 \\cdot \\text{Cosine Similarity} \\;+\\;\n",
        "0.30 \\cdot \\text{Average Rating (scaled)} \\;+\\;\n",
        "0.20 \\cdot \\text{Sentiment (scaled)} \\;+\\;\n",
        "0.10 \\cdot \\text{Review Count (scaled)}\n",
        "$$\n",
        "\n",
        "We then ranked the beers by this score. Based on this, our top 3 recommendations are:\n",
        "1. Triple Shot\n",
        "2. KBS\n",
        "3. Parabajava\n",
        "\n",
        "Other contenders that were listed in the Top-20 ranked table include: Speedway Stout - Bourbon Barrel Aged, Kaggen!, Stormaktsporter, Breakfast Stout, and CBS.\n",
        "\n",
        "We've added in a Top-20 ranked table, with the Top-3 beers flagged, reported cosine similarity, average rating, sentiment, review count, and the composite score, to make the recommendation process transparent.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2UjzK4SILMm"
      },
      "source": [
        "Task C. How would your recommendations change if you use word vectors (e.g., the spaCy package with\n",
        "medium sized pretrained word vectors) instead of the plain vanilla bag-of-words cosine similarity?\n",
        "In your analysis, which method gives you better recommendations in this context? Explain your\n",
        "response.\n",
        "This article may be useful:\n",
        "Word Embeddings Versus Bag-of-Words: The Curious Case of Recommender Systems | by Josh Barua |\n",
        "The Startup | Medium\n",
        "Note that the article doesn’t claim that bag-of-words will always be better than word embeddings for\n",
        "recommender systems. It lays out conditions under which it is likely to be the case. That is, depending\n",
        "on the attributes you use, you may or may not see the same effect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFkF72HnIOay",
        "outputId": "1eeb161a-9d0d-4754-a9d0-e186f4715e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Task C: Word Vectors vs Bag-of-Words Comparison\n",
            "================================================================================\n",
            "Task B baseline found:\n",
            "  Customer attributes: ['chocolate', 'dark', 'coffee']\n",
            "  Beer profiles: 249 beers\n",
            "  TF-IDF recommendations: 243 beers\n",
            "\n",
            "Implementing word vector approach...\n",
            "Loading spaCy medium model (en_core_web_md)...\n",
            "  Vector dimensions: 300\n",
            "  Vocabulary size: 20000\n",
            "Creating customer preference embedding...\n",
            "Creating beer profile embeddings...\n",
            "  Processing beer 1/249...\n",
            "  Processing beer 51/249...\n",
            "  Processing beer 101/249...\n",
            "  Processing beer 151/249...\n",
            "  Processing beer 201/249...\n",
            "Computing word vector similarities...\n",
            "Word vector recommendations generated\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS: HOW RECOMMENDATIONS CHANGED\n",
            "================================================================================\n",
            "Customer request: 'I want a beer that is chocolate, dark, coffee'\n",
            "\n",
            "Overlap Analysis:\n",
            "  Common beers in both top 10s: 5 (50%)\n",
            "  Only in TF-IDF top 10: 5\n",
            "  Only in Word Vectors top 10: 5\n",
            "  Common beers: Triple Shot, CBS (Canadian Breakfast Stout), Coffee Cinnamon B-Bomb...\n",
            "\n",
            "====================================================================================================\n",
            "SIDE-BY-SIDE COMPARISON: TOP 10 RECOMMENDATIONS\n",
            "====================================================================================================\n",
            "\n",
            "TF-IDF BAG-OF-WORDS (Task B Baseline):\n",
            "Rank Beer Name                                     Similarity Score   \n",
            "---------------------------------------------------------------------------\n",
            "1    Triple Shot                                   0.621      0.655 ✓\n",
            "2    KBS                                           0.489      0.631 ✓\n",
            "3    Parabajava                                    0.417      0.583 ✓\n",
            "4    Speedway Stout - Bourbon Barrel-Aged          0.463      0.579  \n",
            "5    Breakfast Stout                               0.471      0.574  \n",
            "6    Kaggen! Stormaktsporter                       0.393      0.574  \n",
            "7    CBS (Canadian Breakfast Stout)                0.356      0.565 ✓\n",
            "8    Coffee Cinnamon B-Bomb                        0.337      0.559 ✓\n",
            "9    Parabola                                      0.338      0.554  \n",
            "10   Speedway Stout - Vietnamese Coffee - Bourbon  0.358      0.552  \n",
            "\n",
            "WORD VECTORS (SpaCy Alternative):\n",
            "Rank Beer Name                                     Similarity Score   \n",
            "---------------------------------------------------------------------------\n",
            "1    KBS                                           0.771      0.744 ✓\n",
            "2    Triple Shot                                   0.835      0.740 ✓\n",
            "3    V.S.O.J.                                      0.744      0.740  \n",
            "4    Kentucky Brunch Brand Stout                   0.756      0.738  \n",
            "5    CBS (Canadian Breakfast Stout)                0.788      0.737 ✓\n",
            "6    Coffee Cinnamon B-Bomb                        0.780      0.737 ✓\n",
            "7    Mornin' Delight                               0.770      0.733  \n",
            "8    Trappist Westvleteren 12 (XII)                0.733      0.733  \n",
            "9    Parabajava                                    0.781      0.729 ✓\n",
            "10   Red Eye November                              0.770      0.729  \n",
            "\n",
            "✓ = Appears in both top 10 lists\n",
            "\n",
            "================================================================================\n",
            "STATISTICAL DIFFERENCES\n",
            "================================================================================\n",
            "Similarity Score Distributions:\n",
            "  TF-IDF Bag-of-Words:  Mean=0.093, Std=0.123\n",
            "  Word Vectors:         Mean=0.673, Std=0.116\n",
            "  Difference in means:  0.580\n",
            "\n",
            "Top Recommendation Comparison:\n",
            "  TF-IDF #1: Triple Shot (similarity: 0.621)\n",
            "  Word Vector #1: KBS (similarity: 0.771)\n",
            "  Same top beer: No\n",
            "\n",
            "================================================================================\n",
            "RESULTS SUMMARY\n",
            "================================================================================\n",
            "Context Analysis for Beer Recommendations:\n",
            "  'chocolate': Found in 117/249 beers (47.0%)\n",
            "  'dark': Found in 161/249 beers (64.7%)\n",
            "  'coffee': Found in 89/249 beers (35.7%)\n",
            "  Average attribute coverage: 49.1%\n",
            "\n",
            "Method Agreement Analysis:\n",
            "  Agreement rate: 50% (Moderate)\n",
            "\n",
            "================================================================================\n",
            "SUMMARY: HOW RECOMMENDATIONS CHANGED\n",
            "================================================================================\n",
            "When switching from TF-IDF bag-of-words to word vectors:\n",
            "  • 5 beers remained in top 10 (50% consistency)\n",
            "  • 5 beers dropped out of top 10\n",
            "  • 5 new beers entered top 10\n",
            "  • Top recommendation changed\n",
            "\n",
            "Task C Complete: Word vectors vs bag-of-words comparison finished!\n",
            "TF-IDF top-3: Triple Shot, KBS, Parabajava\n",
            "Word Vector top-3: KBS, Triple Shot, V.S.O.J.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Task C: Word Vectors vs Bag-of-Words Comparison\n",
        "REQUIRES: Task B to be completed first\n",
        "\n",
        "Question: How would your recommendations change if you use word vectors\n",
        "instead of the plain vanilla bag-of-words cosine similarity?\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Task C: Word Vectors vs Bag-of-Words Comparison\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ================================\n",
        "# STEP 1: VALIDATE TASK B COMPLETION\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    # Check if Task B variables exist\n",
        "    test_vars = [selected_attributes, beer_profiles, recommendations]\n",
        "    print(f\"Task B baseline found:\")\n",
        "    print(f\"  Customer attributes: {selected_attributes}\")\n",
        "    print(f\"  Beer profiles: {len(beer_profiles)} beers\")\n",
        "    print(f\"  TF-IDF recommendations: {len(recommendations)} beers\")\n",
        "except NameError as e:\n",
        "    print(f\"Error: Task B must be completed first!\")\n",
        "    print(f\"Missing variable: {e}\")\n",
        "    print(f\"Please run Task B script before running Task C\")\n",
        "    exit(1)\n",
        "\n",
        "# ================================\n",
        "# STEP 2: IMPLEMENT WORD VECTOR METHOD\n",
        "# ================================\n",
        "\n",
        "print(f\"\\nImplementing word vector approach...\")\n",
        "\n",
        "def load_spacy_model():\n",
        "    \"\"\"Load spaCy model with error handling\"\"\"\n",
        "    try:\n",
        "        print(\"Loading spaCy medium model (en_core_web_md)...\")\n",
        "        nlp = spacy.load(\"en_core_web_md\")\n",
        "        print(f\"  Vector dimensions: {nlp.vocab.vectors_length}\")\n",
        "        print(f\"  Vocabulary size: {len(nlp.vocab.vectors)}\")\n",
        "        return nlp\n",
        "    except IOError:\n",
        "        print(\"Error: spaCy medium model not found!\")\n",
        "        print(\"Install with: python -m spacy download en_core_web_md\")\n",
        "        exit(1)\n",
        "\n",
        "nlp = load_spacy_model()\n",
        "\n",
        "def get_text_embedding(text, nlp_model):\n",
        "    \"\"\"Create document embedding by averaging word vectors\"\"\"\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return np.zeros(nlp_model.vocab.vectors_length)\n",
        "\n",
        "    doc = nlp_model(text)\n",
        "\n",
        "    # Get vectors for meaningful tokens\n",
        "    vectors = []\n",
        "    for token in doc:\n",
        "        if (token.has_vector and\n",
        "            not token.is_stop and\n",
        "            not token.is_punct and\n",
        "            not token.is_space and\n",
        "            len(token.text) > 2):\n",
        "            vectors.append(token.vector)\n",
        "\n",
        "    if vectors:\n",
        "        # Average the word vectors and normalize\n",
        "        embedding = np.mean(vectors, axis=0)\n",
        "        norm = np.linalg.norm(embedding)\n",
        "        if norm > 0:\n",
        "            embedding = embedding / norm\n",
        "        return embedding\n",
        "    else:\n",
        "        return np.zeros(nlp_model.vocab.vectors_length)\n",
        "\n",
        "# Create customer embedding\n",
        "print(\"Creating customer preference embedding...\")\n",
        "customer_profile = ' '.join(selected_attributes * 3)\n",
        "customer_embedding = get_text_embedding(customer_profile, nlp)\n",
        "\n",
        "# Create beer embeddings\n",
        "print(\"Creating beer profile embeddings...\")\n",
        "beer_embeddings = []\n",
        "beer_names = []\n",
        "\n",
        "for i, (beer_name, review_text) in enumerate(beer_profiles['clean_review'].items()):\n",
        "    if i % 50 == 0:\n",
        "        print(f\"  Processing beer {i+1}/{len(beer_profiles)}...\")\n",
        "\n",
        "    embedding = get_text_embedding(review_text, nlp)\n",
        "    beer_embeddings.append(embedding)\n",
        "    beer_names.append(beer_name)\n",
        "\n",
        "beer_embeddings = np.array(beer_embeddings)\n",
        "\n",
        "# Calculate cosine similarities with word vectors\n",
        "print(\"Computing word vector similarities...\")\n",
        "customer_embedding_reshaped = customer_embedding.reshape(1, -1)\n",
        "wv_similarities = cosine_similarity(customer_embedding_reshaped, beer_embeddings).flatten()\n",
        "\n",
        "# Create word vector recommendations\n",
        "wv_recommendations = pd.DataFrame({\n",
        "    'beer_name': beer_names,\n",
        "    'wv_similarity': wv_similarities,\n",
        "    'avg_rating': beer_profiles['user_rating'],\n",
        "    'avg_sentiment': beer_profiles['sentiment'],\n",
        "    'review_count': beer_profiles['review_count']\n",
        "})\n",
        "\n",
        "# Calculate composite score (same weights as Task B)\n",
        "wv_recommendations['wv_composite_score'] = (\n",
        "    0.4 * wv_recommendations['wv_similarity'] +\n",
        "    0.3 * (wv_recommendations['avg_rating'] / 5.0) +\n",
        "    0.2 * ((wv_recommendations['avg_sentiment'] + 1) / 2) +\n",
        "    0.1 * np.log(wv_recommendations['review_count'] + 1) / 10\n",
        ")\n",
        "\n",
        "wv_recommendations = wv_recommendations.sort_values('wv_composite_score', ascending=False)\n",
        "\n",
        "print(f\"Word vector recommendations generated\")\n",
        "\n",
        "# ================================\n",
        "# STEP 3: COMPARE HOW RECOMMENDATIONS CHANGED\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ANALYSIS: HOW RECOMMENDATIONS CHANGED\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f\"Customer request: 'I want a beer that is {', '.join(selected_attributes)}'\")\n",
        "\n",
        "# Get top 10 from both methods\n",
        "tfidf_top10 = recommendations.head(10)\n",
        "wv_top10 = wv_recommendations.head(10)\n",
        "\n",
        "# Find overlaps\n",
        "tfidf_beers = set(tfidf_top10['beer_name'])\n",
        "wv_beers = set(wv_top10['beer_name'])\n",
        "common_beers = tfidf_beers.intersection(wv_beers)\n",
        "tfidf_only = tfidf_beers - wv_beers\n",
        "wv_only = wv_beers - tfidf_beers\n",
        "\n",
        "print(f\"\\nOverlap Analysis:\")\n",
        "print(f\"  Common beers in both top 10s: {len(common_beers)} ({len(common_beers)/10:.0%})\")\n",
        "print(f\"  Only in TF-IDF top 10: {len(tfidf_only)}\")\n",
        "print(f\"  Only in Word Vectors top 10: {len(wv_only)}\")\n",
        "\n",
        "if len(common_beers) > 0:\n",
        "    print(f\"  Common beers: {', '.join(list(common_beers)[:3])}...\")\n",
        "\n",
        "# Side-by-side comparison\n",
        "print(f\"\\n{'='*100}\")\n",
        "print(\"SIDE-BY-SIDE COMPARISON: TOP 10 RECOMMENDATIONS\")\n",
        "print(f\"{'='*100}\")\n",
        "\n",
        "print(f\"\\nTF-IDF BAG-OF-WORDS (Task B Baseline):\")\n",
        "print(f\"{'Rank':<4} {'Beer Name':<45} {'Similarity':<10} {'Score':<8}\")\n",
        "print(\"-\" * 75)\n",
        "for i, (_, row) in enumerate(tfidf_top10.iterrows(), 1):\n",
        "    marker = \"✓\" if row['beer_name'] in common_beers else \" \"\n",
        "    print(f\"{i:<4} {row['beer_name'][:44]:<45} {row['cosine_similarity']:.3f}      {row['composite_score']:.3f} {marker}\")\n",
        "\n",
        "print(f\"\\nWORD VECTORS (SpaCy Alternative):\")\n",
        "print(f\"{'Rank':<4} {'Beer Name':<45} {'Similarity':<10} {'Score':<8}\")\n",
        "print(\"-\" * 75)\n",
        "for i, (_, row) in enumerate(wv_top10.iterrows(), 1):\n",
        "    marker = \"✓\" if row['beer_name'] in common_beers else \" \"\n",
        "    print(f\"{i:<4} {row['beer_name'][:44]:<45} {row['wv_similarity']:.3f}      {row['wv_composite_score']:.3f} {marker}\")\n",
        "\n",
        "print(f\"\\n✓ = Appears in both top 10 lists\")\n",
        "\n",
        "# Statistical comparison\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"STATISTICAL DIFFERENCES\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "tfidf_sim_mean = recommendations['cosine_similarity'].mean()\n",
        "wv_sim_mean = wv_recommendations['wv_similarity'].mean()\n",
        "tfidf_sim_std = recommendations['cosine_similarity'].std()\n",
        "wv_sim_std = wv_recommendations['wv_similarity'].std()\n",
        "\n",
        "print(f\"Similarity Score Distributions:\")\n",
        "print(f\"  TF-IDF Bag-of-Words:  Mean={tfidf_sim_mean:.3f}, Std={tfidf_sim_std:.3f}\")\n",
        "print(f\"  Word Vectors:         Mean={wv_sim_mean:.3f}, Std={wv_sim_std:.3f}\")\n",
        "print(f\"  Difference in means:  {abs(wv_sim_mean - tfidf_sim_mean):.3f}\")\n",
        "\n",
        "# Top recommendation comparison\n",
        "tfidf_top1 = recommendations.iloc[0]\n",
        "wv_top1 = wv_recommendations.iloc[0]\n",
        "\n",
        "print(f\"\\nTop Recommendation Comparison:\")\n",
        "print(f\"  TF-IDF #1: {tfidf_top1['beer_name']} (similarity: {tfidf_top1['cosine_similarity']:.3f})\")\n",
        "print(f\"  Word Vector #1: {wv_top1['beer_name']} (similarity: {wv_top1['wv_similarity']:.3f})\")\n",
        "print(f\"  Same top beer: {'Yes' if tfidf_top1['beer_name'] == wv_top1['beer_name'] else 'No'}\")\n",
        "\n",
        "# ================================\n",
        "# STEP 4: RESULTS SUMMARY\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f\"Context Analysis for Beer Recommendations:\")\n",
        "\n",
        "# Check attribute coverage in the data\n",
        "attribute_coverage = {}\n",
        "for attr in selected_attributes:\n",
        "    count = sum(1 for text in beer_profiles['clean_review'] if attr in text.lower())\n",
        "    coverage = count / len(beer_profiles)\n",
        "    attribute_coverage[attr] = coverage\n",
        "    print(f\"  '{attr}': Found in {count}/{len(beer_profiles)} beers ({coverage:.1%})\")\n",
        "\n",
        "avg_coverage = sum(attribute_coverage.values()) / len(attribute_coverage)\n",
        "print(f\"  Average attribute coverage: {avg_coverage:.1%}\")\n",
        "\n",
        "print(f\"\\nMethod Agreement Analysis:\")\n",
        "agreement_rate = len(common_beers) / 10\n",
        "if agreement_rate >= 0.7:\n",
        "    agreement_level = \"High\"\n",
        "elif agreement_rate >= 0.4:\n",
        "    agreement_level = \"Moderate\"\n",
        "else:\n",
        "    agreement_level = \"Low\"\n",
        "\n",
        "print(f\"  Agreement rate: {agreement_rate:.0%} ({agreement_level})\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"SUMMARY: HOW RECOMMENDATIONS CHANGED\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f\"When switching from TF-IDF bag-of-words to word vectors:\")\n",
        "print(f\"  • {len(common_beers)} beers remained in top 10 ({agreement_rate:.0%} consistency)\")\n",
        "print(f\"  • {len(tfidf_only)} beers dropped out of top 10\")\n",
        "print(f\"  • {len(wv_only)} new beers entered top 10\")\n",
        "print(f\"  • Top recommendation {'stayed the same' if tfidf_top1['beer_name'] == wv_top1['beer_name'] else 'changed'}\")\n",
        "\n",
        "print(f\"\\nTask C Complete: Word vectors vs bag-of-words comparison finished!\")\n",
        "print(f\"TF-IDF top-3: {', '.join(tfidf_top10.head(3)['beer_name'].tolist())}\")\n",
        "print(f\"Word Vector top-3: {', '.join(wv_top10.head(3)['beer_name'].tolist())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDF42pKLD9Tp"
      },
      "source": [
        "In **Task C**, we used the spaCy package's pretrained word embeddings instead of the plain vanilla bag-of-words cosine similarity. Unlike the BoW model (which only looks at the words that appear), the spaCy package places words in a vector space where similar words are close together (“bourbon/vanilla near chocolate/dark”).\n",
        "\n",
        "We then created a document vector for each beer by averaging the vectors of all meaningful words in its reviews, and a query vector for the attributes (chocolate, dark, coffee) in the same way. We then computed cosine similarity and combined it with rating, sentiment, and review count using the same composite weighting as we did in Task B. This let us measure how closely the language in each beer’s reviews matched the customer’s desired attributes.\n",
        "\n",
        "The results showed some overlap between the two methods' Top-10 lists. Results:\n",
        "- Top-3 (TF-IDF BoW): TripleShot, KBS, Parabajava\n",
        "- Top-3 (Word Vectors): KBS, Triple Shot, V.S.O.J\n",
        "- Top-10 overlap: 5/10 beers in common → 50% overlap\n",
        "- #1 recommendation: Changed (relative to Task B); from TripleShot to KBS\n",
        "\n",
        "With the chocolate/dark/coffee, the Bag-of-Words model generates specific keyword matches whereas the word vectors generates/ focuses on reviews that use related or similar descriptors but aren't necessarily 1:1 (i.e barrel/ vanilla). This, in turn, lifted beers such as V.S.O.J into the Top-3 and moved KBS to the number 1 spot. Here, found that both methods are ultimately useful- bag of words makes sure that the top picks are aligned to the stated/top attirbutes, while the word embeddings contextualize the output and add semantic coverage. However, a combination or a hybrid model is likely best suited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1VfO23pq21j"
      },
      "source": [
        "**Task D**: Create custom word embeddings from your product review data instead of using the default SpaCy word embeddings. Do your top-3 recommendations change when using your own embeddings?\n",
        "You can use either SpaCy or Gensim in creating your custom embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv3CD4ilq8Iv",
        "outputId": "ffdd3a87-47ab-417c-e0fd-eb5d5393d9ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Task D: Custom Word Embeddings from Beer Review Data\n",
            "================================================================================\n",
            "Previous tasks found:\n",
            "  Task B (TF-IDF): 243 recommendations\n",
            "  Task C (spaCy): 249 recommendations\n",
            "  Customer attributes: ['chocolate', 'dark', 'coffee']\n",
            "\n",
            "Preparing text data for custom embedding training...\n",
            "Processing beer review text...\n",
            "  Processing beer 1/249...\n",
            "  Processing beer 101/249...\n",
            "  Processing beer 201/249...\n",
            "Training data prepared:\n",
            "  Total sentences: 249\n",
            "  Total words: 153930\n",
            "  Average words per sentence: 618.2\n",
            "\n",
            "Training custom Word2Vec embeddings...\n",
            "Word2Vec parameters:\n",
            "  Vector size: 100\n",
            "  Context window: 5\n",
            "  Min word count: 5\n",
            "  Training epochs: 20\n",
            "Custom embeddings trained:\n",
            "  Vocabulary size: 2689\n",
            "  Vector dimensions: 100\n",
            "\n",
            "Customer attribute coverage in custom embeddings:\n",
            "  ✓ 'chocolate': Found in vocabulary\n",
            "  ✓ 'dark': Found in vocabulary\n",
            "  ✓ 'coffee': Found in vocabulary\n",
            "\n",
            "Creating document embeddings using custom Word2Vec...\n",
            "Creating customer preference embedding...\n",
            "Creating beer profile embeddings...\n",
            "  Processing beer 1/249...\n",
            "  Processing beer 51/249...\n",
            "  Processing beer 101/249...\n",
            "  Processing beer 151/249...\n",
            "  Processing beer 201/249...\n",
            "Computing similarities with custom embeddings...\n",
            "\n",
            "Generating recommendations with custom embeddings...\n",
            "Custom embedding recommendations generated\n",
            "\n",
            "================================================================================\n",
            "COMPARISON: DO TOP-3 RECOMMENDATIONS CHANGE?\n",
            "================================================================================\n",
            "Customer request: 'I want a beer that is chocolate, dark, coffee'\n",
            "\n",
            "TOP-3 COMPARISON ACROSS ALL METHODS:\n",
            "Method               #1 Recommendation              #2 Recommendation              #3 Recommendation             \n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "TF-IDF (Task B)      Triple Shot                    KBS                            Parabajava                    \n",
            "spaCy Vectors (Task C) KBS                            Triple Shot                    V.S.O.J.                      \n",
            "Custom Embeddings    V.S.O.J.                       CBS (Canadian Breakfast Stout  Abraxas - Barrel-Aged         \n",
            "\n",
            "================================================================================\n",
            "CHANGE ANALYSIS\n",
            "================================================================================\n",
            "Custom Embeddings vs spaCy Pre-trained:\n",
            "  Common beers in top-3: 1/3\n",
            "  Overlap percentage: 33%\n",
            "\n",
            "Custom Embeddings vs TF-IDF:\n",
            "  Common beers in top-3: 0/3\n",
            "  Overlap percentage: 0%\n",
            "\n",
            "================================================================================\n",
            "DETAILED TOP-3 WITH CUSTOM EMBEDDINGS\n",
            "================================================================================\n",
            "\n",
            "1. V.S.O.J. (also in: spaCy)\n",
            "   Custom Similarity: 0.714\n",
            "   Average Rating: 4.69/5.0\n",
            "   Sentiment Score: 0.380\n",
            "   Final Score: 0.728\n",
            "\n",
            "2. CBS (Canadian Breakfast Stout) (unique to custom)\n",
            "   Custom Similarity: 0.757\n",
            "   Average Rating: 4.52/5.0\n",
            "   Sentiment Score: 0.247\n",
            "   Final Score: 0.725\n",
            "\n",
            "3. Abraxas - Barrel-Aged (unique to custom)\n",
            "   Custom Similarity: 0.723\n",
            "   Average Rating: 4.66/5.0\n",
            "   Sentiment Score: 0.265\n",
            "   Final Score: 0.720\n",
            "\n",
            "================================================================================\n",
            "CUSTOM EMBEDDING QUALITY ANALYSIS\n",
            "================================================================================\n",
            "Similarity Score Statistics:\n",
            "  Custom Embeddings: Mean=0.565, Std=0.122\n",
            "  spaCy Pre-trained: Mean=0.673, Std=0.116\n",
            "  Difference: 0.108\n",
            "\n",
            "Custom Embedding Quality Check:\n",
            "  'chocolate' similar to: joy, chip, sugars\n",
            "  'dark' similar to: brown, cola, deep\n",
            "  'coffee' similar to: chocolate, espresso, smoky\n",
            "\n",
            "Quality Assessment: Good\n",
            "  Large vocabulary (2689 words) should provide good coverage\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULTS\n",
            "================================================================================\n",
            "Answer: Do top-3 recommendations change when using custom embeddings?\n",
            "  Major changes - 2 out of 3 recommendations changed\n",
            "\n",
            "Task D Complete: Custom embeddings analysis finished!\n",
            "Custom top-3: V.S.O.J., CBS (Canadian Breakfast Stout), Abraxas - Barrel-Aged\n",
            "spaCy top-3: KBS, Triple Shot, V.S.O.J.\n",
            "TF-IDF top-3: Triple Shot, KBS, Parabajava\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Task D: Create custom word embeddings from product review data\n",
        "REQUIRES: Tasks B and C to be completed first\n",
        "\n",
        "Question: Do your top-3 recommendations change when using your own embeddings?\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Task D: Custom Word Embeddings from Beer Review Data\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ================================\n",
        "# STEP 1: VALIDATE PREVIOUS TASKS\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    # Check if previous task variables exist\n",
        "    test_vars = [selected_attributes, beer_profiles, recommendations, wv_recommendations]\n",
        "    print(\"Previous tasks found:\")\n",
        "    print(f\"  Task B (TF-IDF): {len(recommendations)} recommendations\")\n",
        "    print(f\"  Task C (spaCy): {len(wv_recommendations)} recommendations\")\n",
        "    print(f\"  Customer attributes: {selected_attributes}\")\n",
        "except NameError as e:\n",
        "    print(f\"Error: Previous tasks must be completed first!\")\n",
        "    print(f\"Missing variable: {e}\")\n",
        "    print(\"Please run Tasks B and C before running Task D\")\n",
        "    exit(1)\n",
        "\n",
        "# ================================\n",
        "# STEP 2: PREPARE TEXT DATA FOR TRAINING\n",
        "# ================================\n",
        "\n",
        "print(f\"\\nPreparing text data for custom embedding training...\")\n",
        "\n",
        "def prepare_sentences_for_training(beer_profiles):\n",
        "    \"\"\"\n",
        "    Prepare beer review text for Word2Vec training\n",
        "    Tokenizes and cleans text into sentences of words\n",
        "    \"\"\"\n",
        "\n",
        "    def clean_and_tokenize(text):\n",
        "        \"\"\"Clean text and return list of words\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return []\n",
        "\n",
        "        # Convert to lowercase and extract words\n",
        "        text = text.lower()\n",
        "        # Keep only alphabetic words of reasonable length\n",
        "        words = re.findall(r'\\b[a-z]{2,}\\b', text)\n",
        "\n",
        "        # Filter out very common stop words (keep beer-relevant ones)\n",
        "        basic_stops = {'the', 'and', 'but', 'for', 'are', 'was', 'were', 'been',\n",
        "                      'have', 'has', 'had', 'this', 'that', 'with', 'from', 'they',\n",
        "                      'them', 'than', 'very', 'more', 'most', 'some', 'what', 'when'}\n",
        "\n",
        "        words = [word for word in words if word not in basic_stops and len(word) >= 3]\n",
        "\n",
        "        return words\n",
        "\n",
        "    # Collect all sentences from beer reviews\n",
        "    sentences = []\n",
        "    total_words = 0\n",
        "\n",
        "    print(\"Processing beer review text...\")\n",
        "    for i, (beer_name, review_text) in enumerate(beer_profiles['clean_review'].items()):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"  Processing beer {i+1}/{len(beer_profiles)}...\")\n",
        "\n",
        "        # Split long review into smaller chunks (Word2Vec works better with shorter sentences)\n",
        "        if isinstance(review_text, str) and len(review_text) > 100:\n",
        "            # Split by sentence-ending punctuation\n",
        "            chunks = re.split(r'[.!?]+', review_text)\n",
        "\n",
        "            for chunk in chunks:\n",
        "                words = clean_and_tokenize(chunk)\n",
        "                if len(words) >= 5:  # Minimum sentence length\n",
        "                    sentences.append(words)\n",
        "                    total_words += len(words)\n",
        "\n",
        "    print(f\"Training data prepared:\")\n",
        "    print(f\"  Total sentences: {len(sentences)}\")\n",
        "    print(f\"  Total words: {total_words}\")\n",
        "    print(f\"  Average words per sentence: {total_words/len(sentences):.1f}\")\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Prepare training data\n",
        "training_sentences = prepare_sentences_for_training(beer_profiles)\n",
        "\n",
        "if len(training_sentences) < 100:\n",
        "    print(\"Warning: Very small training corpus. Custom embeddings may be poor quality.\")\n",
        "    print(\"Consider using pre-trained embeddings for better results.\")\n",
        "\n",
        "# ================================\n",
        "# STEP 3: TRAIN CUSTOM WORD2VEC EMBEDDINGS\n",
        "# ================================\n",
        "\n",
        "print(f\"\\nTraining custom Word2Vec embeddings...\")\n",
        "\n",
        "def train_custom_embeddings(sentences, vector_size=100, window=5, min_count=5, epochs=20):\n",
        "    \"\"\"\n",
        "    Train custom Word2Vec embeddings on beer review data\n",
        "\n",
        "    Parameters:\n",
        "    - vector_size: Dimensionality of word vectors\n",
        "    - window: Context window size\n",
        "    - min_count: Ignore words with frequency < min_count\n",
        "    - epochs: Number of training iterations\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Word2Vec parameters:\")\n",
        "    print(f\"  Vector size: {vector_size}\")\n",
        "    print(f\"  Context window: {window}\")\n",
        "    print(f\"  Min word count: {min_count}\")\n",
        "    print(f\"  Training epochs: {epochs}\")\n",
        "\n",
        "    # Train Word2Vec model\n",
        "    model = Word2Vec(\n",
        "        sentences=sentences,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        workers=4,\n",
        "        epochs=epochs,\n",
        "        sg=1,  # Skip-gram model (better for small datasets)\n",
        "        seed=42  # For reproducibility\n",
        "    )\n",
        "\n",
        "    print(f\"Custom embeddings trained:\")\n",
        "    print(f\"  Vocabulary size: {len(model.wv.key_to_index)}\")\n",
        "    print(f\"  Vector dimensions: {model.wv.vector_size}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train the custom embedding model\n",
        "custom_w2v_model = train_custom_embeddings(training_sentences)\n",
        "\n",
        "# Check if customer attributes are in vocabulary\n",
        "print(f\"\\nCustomer attribute coverage in custom embeddings:\")\n",
        "missing_attributes = []\n",
        "for attr in selected_attributes:\n",
        "    if attr in custom_w2v_model.wv.key_to_index:\n",
        "        print(f\"  ✓ '{attr}': Found in vocabulary\")\n",
        "    else:\n",
        "        print(f\"  ✗ '{attr}': Missing from vocabulary\")\n",
        "        missing_attributes.append(attr)\n",
        "\n",
        "if missing_attributes:\n",
        "    print(f\"Warning: {len(missing_attributes)} customer attributes missing from vocabulary\")\n",
        "    print(\"This may affect recommendation quality\")\n",
        "\n",
        "# ================================\n",
        "# STEP 4: CREATE DOCUMENT EMBEDDINGS\n",
        "# ================================\n",
        "\n",
        "print(f\"\\nCreating document embeddings using custom Word2Vec...\")\n",
        "\n",
        "def get_custom_text_embedding(text, w2v_model):\n",
        "    \"\"\"\n",
        "    Create document embedding using custom Word2Vec model\n",
        "    Average word vectors for words in vocabulary\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return np.zeros(w2v_model.wv.vector_size)\n",
        "\n",
        "    # Tokenize text\n",
        "    words = re.findall(r'\\b[a-z]{3,}\\b', text.lower())\n",
        "\n",
        "    # Get vectors for words in vocabulary\n",
        "    vectors = []\n",
        "    for word in words:\n",
        "        if word in w2v_model.wv.key_to_index:\n",
        "            vectors.append(w2v_model.wv[word])\n",
        "\n",
        "    if vectors:\n",
        "        # Average word vectors and normalize\n",
        "        embedding = np.mean(vectors, axis=0)\n",
        "        norm = np.linalg.norm(embedding)\n",
        "        if norm > 0:\n",
        "            embedding = embedding / norm\n",
        "        return embedding\n",
        "    else:\n",
        "        return np.zeros(w2v_model.wv.vector_size)\n",
        "\n",
        "# Create customer embedding with custom model\n",
        "print(\"Creating customer preference embedding...\")\n",
        "customer_profile = ' '.join(selected_attributes * 3)\n",
        "custom_customer_embedding = get_custom_text_embedding(customer_profile, custom_w2v_model)\n",
        "\n",
        "# Create beer embeddings with custom model\n",
        "print(\"Creating beer profile embeddings...\")\n",
        "custom_beer_embeddings = []\n",
        "custom_beer_names = []\n",
        "\n",
        "for i, (beer_name, review_text) in enumerate(beer_profiles['clean_review'].items()):\n",
        "    if i % 50 == 0:\n",
        "        print(f\"  Processing beer {i+1}/{len(beer_profiles)}...\")\n",
        "\n",
        "    embedding = get_custom_text_embedding(review_text, custom_w2v_model)\n",
        "    custom_beer_embeddings.append(embedding)\n",
        "    custom_beer_names.append(beer_name)\n",
        "\n",
        "custom_beer_embeddings = np.array(custom_beer_embeddings)\n",
        "\n",
        "# Calculate similarities\n",
        "print(\"Computing similarities with custom embeddings...\")\n",
        "custom_customer_reshaped = custom_customer_embedding.reshape(1, -1)\n",
        "custom_similarities = cosine_similarity(custom_customer_reshaped, custom_beer_embeddings).flatten()\n",
        "\n",
        "# ================================\n",
        "# STEP 5: GENERATE CUSTOM EMBEDDING RECOMMENDATIONS\n",
        "# ================================\n",
        "\n",
        "print(f\"\\nGenerating recommendations with custom embeddings...\")\n",
        "\n",
        "# Create custom embedding recommendations\n",
        "custom_recommendations = pd.DataFrame({\n",
        "    'beer_name': custom_beer_names,\n",
        "    'custom_similarity': custom_similarities,\n",
        "    'avg_rating': beer_profiles['user_rating'],\n",
        "    'avg_sentiment': beer_profiles['sentiment'],\n",
        "    'review_count': beer_profiles['review_count']\n",
        "})\n",
        "\n",
        "# Calculate composite score (same weights as other methods)\n",
        "custom_recommendations['custom_composite_score'] = (\n",
        "    0.4 * custom_recommendations['custom_similarity'] +\n",
        "    0.3 * (custom_recommendations['avg_rating'] / 5.0) +\n",
        "    0.2 * ((custom_recommendations['avg_sentiment'] + 1) / 2) +\n",
        "    0.1 * np.log(custom_recommendations['review_count'] + 1) / 10\n",
        ")\n",
        "\n",
        "# Sort by composite score\n",
        "custom_recommendations = custom_recommendations.sort_values('custom_composite_score', ascending=False)\n",
        "\n",
        "print(f\"Custom embedding recommendations generated\")\n",
        "\n",
        "# ================================\n",
        "# STEP 6: COMPARE TOP-3 RECOMMENDATIONS\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"COMPARISON: DO TOP-3 RECOMMENDATIONS CHANGE?\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Get top 3 from all methods\n",
        "tfidf_top3 = recommendations.head(3)\n",
        "spacy_top3 = wv_recommendations.head(3)\n",
        "custom_top3 = custom_recommendations.head(3)\n",
        "\n",
        "print(f\"Customer request: 'I want a beer that is {', '.join(selected_attributes)}'\")\n",
        "\n",
        "print(f\"\\nTOP-3 COMPARISON ACROSS ALL METHODS:\")\n",
        "print(f\"{'Method':<20} {'#1 Recommendation':<30} {'#2 Recommendation':<30} {'#3 Recommendation':<30}\")\n",
        "print(\"-\" * 110)\n",
        "\n",
        "methods_data = [\n",
        "    (\"TF-IDF (Task B)\", tfidf_top3),\n",
        "    (\"spaCy Vectors (Task C)\", spacy_top3),\n",
        "    (\"Custom Embeddings\", custom_top3)\n",
        "]\n",
        "\n",
        "for method_name, top3_df in methods_data:\n",
        "    rec1 = top3_df.iloc[0]['beer_name'][:29]\n",
        "    rec2 = top3_df.iloc[1]['beer_name'][:29]\n",
        "    rec3 = top3_df.iloc[2]['beer_name'][:29]\n",
        "    print(f\"{method_name:<20} {rec1:<30} {rec2:<30} {rec3:<30}\")\n",
        "\n",
        "# Analyze changes\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"CHANGE ANALYSIS\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Compare custom vs spaCy (most relevant comparison)\n",
        "spacy_top3_names = set(spacy_top3['beer_name'])\n",
        "custom_top3_names = set(custom_top3['beer_name'])\n",
        "tfidf_top3_names = set(tfidf_top3['beer_name'])\n",
        "\n",
        "spacy_custom_overlap = spacy_top3_names.intersection(custom_top3_names)\n",
        "tfidf_custom_overlap = tfidf_top3_names.intersection(custom_top3_names)\n",
        "\n",
        "print(f\"Custom Embeddings vs spaCy Pre-trained:\")\n",
        "print(f\"  Common beers in top-3: {len(spacy_custom_overlap)}/3\")\n",
        "print(f\"  Overlap percentage: {len(spacy_custom_overlap)/3:.0%}\")\n",
        "\n",
        "print(f\"\\nCustom Embeddings vs TF-IDF:\")\n",
        "print(f\"  Common beers in top-3: {len(tfidf_custom_overlap)}/3\")\n",
        "print(f\"  Overlap percentage: {len(tfidf_custom_overlap)/3:.0%}\")\n",
        "\n",
        "# Show detailed top-3 from custom embeddings\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"DETAILED TOP-3 WITH CUSTOM EMBEDDINGS\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "for i, (_, beer) in enumerate(custom_top3.iterrows(), 1):\n",
        "    status_indicators = []\n",
        "    if beer['beer_name'] in spacy_top3_names:\n",
        "        status_indicators.append(\"spaCy\")\n",
        "    if beer['beer_name'] in tfidf_top3_names:\n",
        "        status_indicators.append(\"TF-IDF\")\n",
        "\n",
        "    status = \" (also in: \" + \", \".join(status_indicators) + \")\" if status_indicators else \" (unique to custom)\"\n",
        "\n",
        "    print(f\"\\n{i}. {beer['beer_name']}{status}\")\n",
        "    print(f\"   Custom Similarity: {beer['custom_similarity']:.3f}\")\n",
        "    print(f\"   Average Rating: {beer['avg_rating']:.2f}/5.0\")\n",
        "    print(f\"   Sentiment Score: {beer['avg_sentiment']:.3f}\")\n",
        "    print(f\"   Final Score: {beer['custom_composite_score']:.3f}\")\n",
        "\n",
        "# ================================\n",
        "# STEP 7: ANALYZE EMBEDDING QUALITY\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"CUSTOM EMBEDDING QUALITY ANALYSIS\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Check similarity statistics\n",
        "custom_sim_mean = custom_recommendations['custom_similarity'].mean()\n",
        "custom_sim_std = custom_recommendations['custom_similarity'].std()\n",
        "spacy_sim_mean = wv_recommendations['wv_similarity'].mean()\n",
        "spacy_sim_std = wv_recommendations['wv_similarity'].std()\n",
        "\n",
        "print(f\"Similarity Score Statistics:\")\n",
        "print(f\"  Custom Embeddings: Mean={custom_sim_mean:.3f}, Std={custom_sim_std:.3f}\")\n",
        "print(f\"  spaCy Pre-trained: Mean={spacy_sim_mean:.3f}, Std={spacy_sim_std:.3f}\")\n",
        "print(f\"  Difference: {abs(custom_sim_mean - spacy_sim_mean):.3f}\")\n",
        "\n",
        "# Test custom embedding quality with beer terms\n",
        "print(f\"\\nCustom Embedding Quality Check:\")\n",
        "\n",
        "if len(custom_w2v_model.wv.key_to_index) >= 50:\n",
        "    # Find similar words to customer attributes (if they exist)\n",
        "    for attr in selected_attributes:\n",
        "        if attr in custom_w2v_model.wv.key_to_index:\n",
        "            try:\n",
        "                similar_words = custom_w2v_model.wv.most_similar(attr, topn=3)\n",
        "                similar_list = [word for word, score in similar_words]\n",
        "                print(f\"  '{attr}' similar to: {', '.join(similar_list)}\")\n",
        "            except:\n",
        "                print(f\"  '{attr}': Cannot find similar words\")\n",
        "        else:\n",
        "            print(f\"  '{attr}': Not in custom vocabulary\")\n",
        "\n",
        "    # Check vocabulary coverage\n",
        "    vocab_size = len(custom_w2v_model.wv.key_to_index)\n",
        "    if vocab_size < 500:\n",
        "        quality_assessment = \"Limited\"\n",
        "        print(f\"\\nQuality Assessment: {quality_assessment}\")\n",
        "        print(f\"  Small vocabulary ({vocab_size} words) may limit recommendation quality\")\n",
        "    elif vocab_size < 2000:\n",
        "        quality_assessment = \"Moderate\"\n",
        "        print(f\"\\nQuality Assessment: {quality_assessment}\")\n",
        "        print(f\"  Reasonable vocabulary ({vocab_size} words) for domain-specific embeddings\")\n",
        "    else:\n",
        "        quality_assessment = \"Good\"\n",
        "        print(f\"\\nQuality Assessment: {quality_assessment}\")\n",
        "        print(f\"  Large vocabulary ({vocab_size} words) should provide good coverage\")\n",
        "else:\n",
        "    quality_assessment = \"Poor\"\n",
        "    print(f\"\\nQuality Assessment: {quality_assessment}\")\n",
        "    print(\"Very small vocabulary - embeddings may not be reliable\")\n",
        "\n",
        "# ================================\n",
        "# STEP 8: FINAL RESULTS\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"FINAL RESULTS\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f\"Answer: Do top-3 recommendations change when using custom embeddings?\")\n",
        "\n",
        "if len(spacy_custom_overlap) == 3:\n",
        "    change_assessment = \"No - all 3 recommendations remain the same\"\n",
        "elif len(spacy_custom_overlap) >= 2:\n",
        "    change_assessment = f\"Minor changes - {3-len(spacy_custom_overlap)} out of 3 recommendations changed\"\n",
        "else:\n",
        "    change_assessment = f\"Major changes - {3-len(spacy_custom_overlap)} out of 3 recommendations changed\"\n",
        "\n",
        "print(f\"  {change_assessment}\")\n",
        "\n",
        "print(f\"\\nTask D Complete: Custom embeddings analysis finished!\")\n",
        "print(f\"Custom top-3: {', '.join(custom_top3['beer_name'].tolist())}\")\n",
        "print(f\"spaCy top-3: {', '.join(spacy_top3['beer_name'].tolist())}\")\n",
        "print(f\"TF-IDF top-3: {', '.join(tfidf_top3['beer_name'].tolist())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **Task D**, we created custom word embeddings from our product review data instead of using the default SpaCY word embeddings. Since these were from our dataset, they capture beer-specific semantics and language patterns.\n",
        "\n",
        "Similar to Task B and C, we created a vector for each beer (by averaging its review word vectors) and a query vector for the customer’s attributes (chocolate, dark, coffee). We then computed cosine similarity and combined it with rating, sentiment, and review count using the same composite formula from Task B.\n",
        "\n",
        "The results:\n",
        "\n",
        "Top-3 (TF-IDF BoW): Triple Shot, KBS, Parabajava\n",
        "\n",
        "Top-3 (spaCy vectors): KBS, Triple Shot, V.S.O.J.\n",
        "\n",
        "Top-3 (Custom Word2Vec): V.S.O.J., CBS (Canadian Breakfast Stout), Abraxas – Barrel-Aged\n",
        "\n",
        "Overlap with spaCy: 1/3\n",
        "\n",
        "Overlap with TF-IDF: 0/3\n",
        "\n",
        "The #1 recommendation: This changed again, this time to V.S.O.J.\n",
        "\n",
        "Overall, the custom embeddings surfaced different beers because they focused on how words are actually used in beer reviews and focused on the context. Compared to BoW and spaCy, this method pushed V.S.O.J., CBS, and Abraxas – Barrel-Aged into the top results, showing that training on domain data can meaningfully change recommendations."
      ],
      "metadata": {
        "id": "RPjCkK9HKWB7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfg1JMCUriNm"
      },
      "source": [
        "Task E. How would your recommendations differ if you ignored the similarity and feature sentiment\n",
        "scores and simply chose the 3 highest rated products from your entire dataset? Would these products\n",
        "meet the requirements of the user looking for recommendations? Why or why not? Justify your answer\n",
        "with analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auX0G0Isrh8o",
        "outputId": "d4866a61-17c7-46b4-8bdd-23e16d563ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Task E: Highest Rated vs Personalized Recommendations\n",
            "============================================================\n",
            "Previous tasks found:\n",
            "  Customer attributes: ['chocolate', 'dark', 'coffee']\n",
            "  Personalized recommendations: 243 beers\n",
            "\n",
            "Creating highest-rated recommendations...\n",
            "Highest-rated approach: Simply sort by rating and take top 3\n",
            "\n",
            "================================================================================\n",
            "COMPARISON: PERSONALIZED vs HIGHEST-RATED\n",
            "================================================================================\n",
            "Customer request: 'I want a beer that is chocolate, dark, coffee'\n",
            "\n",
            "Overlap: 0/3 beers appear in both lists (0%)\n",
            "\n",
            "PERSONALIZED TOP 3 (considers customer preferences):\n",
            "Rank Beer Name                                Rating   Similarity\n",
            "----------------------------------------------------------------------\n",
            "1    Triple Shot                              4.50     0.621  \n",
            "2    KBS                                      4.65     0.489  \n",
            "3    Parabajava                               4.55     0.417  \n",
            "\n",
            "HIGHEST-RATED TOP 3 (ignores customer preferences):\n",
            "Rank Beer Name                                Rating   Reviews \n",
            "----------------------------------------------------------------------\n",
            "1    10 Year Barleywine                       5.00     4         \n",
            "2    M.J.K.                                   4.94     8         \n",
            "3    O.W.K.                                   4.88     4         \n",
            "\n",
            "✓ = Appears in both lists\n",
            "\n",
            "============================================================\n",
            "ATTRIBUTE MATCHING ANALYSIS\n",
            "============================================================\n",
            "Customer attribute mentions ('chocolate, dark, coffee'):\n",
            "\n",
            "Personalized recommendations:\n",
            "  Triple Shot: 18 mentions\n",
            "  KBS: 26 mentions\n",
            "  Parabajava: 32 mentions\n",
            "\n",
            "Highest-rated recommendations:\n",
            "  10 Year Barleywine: 0 mentions\n",
            "  M.J.K.: 0 mentions\n",
            "  O.W.K.: 2 mentions\n",
            "\n",
            "Average attribute mentions:\n",
            "  Personalized: 25.3\n",
            "  Highest-rated: 0.7\n",
            "  Difference: +24.7\n",
            "\n",
            "============================================================\n",
            "RATING COMPARISON\n",
            "============================================================\n",
            "Average ratings:\n",
            "  Personalized: 4.57/5.0\n",
            "  Highest-rated: 4.94/5.0\n",
            "  Rating sacrifice: +0.37\n",
            "\n",
            "============================================================\n",
            "ANALYSIS SUMMARY\n",
            "============================================================\n",
            "Question: Would highest-rated products meet user requirements?\n",
            "\n",
            "Findings:\n",
            "  Overlap: 0/3 beers are the same\n",
            "  Attribute relevance: very poor\n",
            "  Rating difference: +0.37 points\n",
            "\n",
            "Conclusion: Highest-rated approach would provide very poor relevance to user requirements\n",
            "\n",
            "Task E Complete!\n",
            "Personalized top beer: Triple Shot\n",
            "Highest-rated top beer: 10 Year Barleywine\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Task E: Compare personalized recommendations vs highest-rated products\n",
        "\n",
        "Question: How would your recommendations differ if you ignored similarity and sentiment\n",
        "and simply chose the 3 highest rated products? Would these meet user requirements?\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Task E: Highest Rated vs Personalized Recommendations\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ================================\n",
        "# STEP 1: VALIDATE PREVIOUS TASKS\n",
        "# ================================\n",
        "\n",
        "try:\n",
        "    test_vars = [selected_attributes, beer_profiles, recommendations]\n",
        "    print(\"Previous tasks found:\")\n",
        "    print(f\"  Customer attributes: {selected_attributes}\")\n",
        "    print(f\"  Personalized recommendations: {len(recommendations)} beers\")\n",
        "except NameError as e:\n",
        "    print(f\"Error: Previous tasks must be completed first!\")\n",
        "    exit(1)\n",
        "\n",
        "# ================================\n",
        "# STEP 2: CREATE HIGHEST-RATED RECOMMENDATIONS\n",
        "# ================================\n",
        "\n",
        "print(f\"\\nCreating highest-rated recommendations...\")\n",
        "\n",
        "# Simple approach: sort by rating, take top 3\n",
        "highest_rated = beer_profiles.sort_values('user_rating', ascending=False)\n",
        "highest_rated_top3 = highest_rated.head(3)\n",
        "\n",
        "print(f\"Highest-rated approach: Simply sort by rating and take top 3\")\n",
        "\n",
        "# ================================\n",
        "# STEP 3: COMPARISON\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"COMPARISON: PERSONALIZED vs HIGHEST-RATED\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f\"Customer request: 'I want a beer that is {', '.join(selected_attributes)}'\")\n",
        "\n",
        "# Get personalized top 3\n",
        "personalized_top3 = recommendations.head(3)\n",
        "\n",
        "# Check overlap\n",
        "personalized_names = set(personalized_top3['beer_name'])\n",
        "highest_rated_names = set(highest_rated_top3.index)\n",
        "overlap = personalized_names.intersection(highest_rated_names)\n",
        "\n",
        "print(f\"\\nOverlap: {len(overlap)}/3 beers appear in both lists ({len(overlap)/3:.0%})\")\n",
        "if len(overlap) > 0:\n",
        "    print(f\"Common beers: {', '.join(overlap)}\")\n",
        "\n",
        "# Side-by-side comparison\n",
        "print(f\"\\nPERSONALIZED TOP 3 (considers customer preferences):\")\n",
        "print(f\"{'Rank':<4} {'Beer Name':<40} {'Rating':<8} {'Similarity':<10}\")\n",
        "print(\"-\" * 70)\n",
        "for i, (_, row) in enumerate(personalized_top3.iterrows(), 1):\n",
        "    marker = \"✓\" if row['beer_name'] in overlap else \" \"\n",
        "    print(f\"{i:<4} {row['beer_name'][:39]:<40} {row['avg_rating']:.2f}     {row['cosine_similarity']:.3f} {marker}\")\n",
        "\n",
        "print(f\"\\nHIGHEST-RATED TOP 3 (ignores customer preferences):\")\n",
        "print(f\"{'Rank':<4} {'Beer Name':<40} {'Rating':<8} {'Reviews':<8}\")\n",
        "print(\"-\" * 70)\n",
        "for i, (beer_name, row) in enumerate(highest_rated_top3.iterrows(), 1):\n",
        "    marker = \"✓\" if beer_name in overlap else \" \"\n",
        "    print(f\"{i:<4} {beer_name[:39]:<40} {row['user_rating']:.2f}     {int(row['review_count']):<8} {marker}\")\n",
        "\n",
        "print(f\"\\n✓ = Appears in both lists\")\n",
        "\n",
        "# ================================\n",
        "# STEP 4: ATTRIBUTE MATCHING ANALYSIS\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ATTRIBUTE MATCHING ANALYSIS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "def count_attribute_mentions(beer_names, beer_profiles, attributes):\n",
        "    \"\"\"Count how often customer attributes appear in beer reviews\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for beer_name in beer_names:\n",
        "        if beer_name in beer_profiles.index:\n",
        "            review_text = beer_profiles.loc[beer_name, 'clean_review']\n",
        "            total_matches = 0\n",
        "\n",
        "            for attr in attributes:\n",
        "                count = review_text.lower().count(attr.lower())\n",
        "                total_matches += count\n",
        "\n",
        "            results.append({\n",
        "                'beer_name': beer_name,\n",
        "                'total_matches': total_matches\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Analyze attribute matching\n",
        "personalized_matches = count_attribute_mentions(\n",
        "    personalized_top3['beer_name'].tolist(),\n",
        "    beer_profiles,\n",
        "    selected_attributes\n",
        ")\n",
        "\n",
        "highest_rated_matches = count_attribute_mentions(\n",
        "    highest_rated_top3.index.tolist(),\n",
        "    beer_profiles,\n",
        "    selected_attributes\n",
        ")\n",
        "\n",
        "print(f\"Customer attribute mentions ('{', '.join(selected_attributes)}'):\")\n",
        "\n",
        "print(f\"\\nPersonalized recommendations:\")\n",
        "for result in personalized_matches:\n",
        "    print(f\"  {result['beer_name'][:35]}: {result['total_matches']} mentions\")\n",
        "\n",
        "print(f\"\\nHighest-rated recommendations:\")\n",
        "for result in highest_rated_matches:\n",
        "    print(f\"  {result['beer_name'][:35]}: {result['total_matches']} mentions\")\n",
        "\n",
        "# Calculate averages\n",
        "personalized_avg = np.mean([r['total_matches'] for r in personalized_matches])\n",
        "highest_rated_avg = np.mean([r['total_matches'] for r in highest_rated_matches])\n",
        "\n",
        "print(f\"\\nAverage attribute mentions:\")\n",
        "print(f\"  Personalized: {personalized_avg:.1f}\")\n",
        "print(f\"  Highest-rated: {highest_rated_avg:.1f}\")\n",
        "print(f\"  Difference: {personalized_avg - highest_rated_avg:+.1f}\")\n",
        "\n",
        "# ================================\n",
        "# STEP 5: RATING COMPARISON\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"RATING COMPARISON\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "personalized_ratings = personalized_top3['avg_rating']\n",
        "highest_rated_ratings = highest_rated_top3['user_rating']\n",
        "\n",
        "print(f\"Average ratings:\")\n",
        "print(f\"  Personalized: {personalized_ratings.mean():.2f}/5.0\")\n",
        "print(f\"  Highest-rated: {highest_rated_ratings.mean():.2f}/5.0\")\n",
        "print(f\"  Rating sacrifice: {highest_rated_ratings.mean() - personalized_ratings.mean():+.2f}\")\n",
        "\n",
        "# ================================\n",
        "# STEP 6: SIMPLE CONCLUSION\n",
        "# ================================\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ANALYSIS SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "print(f\"Question: Would highest-rated products meet user requirements?\")\n",
        "\n",
        "# Simple decision logic\n",
        "if highest_rated_avg >= personalized_avg * 0.8:\n",
        "    relevance_assessment = \"adequate\"\n",
        "elif highest_rated_avg >= personalized_avg * 0.5:\n",
        "    relevance_assessment = \"poor\"\n",
        "else:\n",
        "    relevance_assessment = \"very poor\"\n",
        "\n",
        "rating_sacrifice = highest_rated_ratings.mean() - personalized_ratings.mean()\n",
        "\n",
        "print(f\"\\nFindings:\")\n",
        "print(f\"  Overlap: {len(overlap)}/3 beers are the same\")\n",
        "print(f\"  Attribute relevance: {relevance_assessment}\")\n",
        "print(f\"  Rating difference: {rating_sacrifice:+.2f} points\")\n",
        "\n",
        "print(f\"\\nConclusion: Highest-rated approach would provide {relevance_assessment} relevance to user requirements\")\n",
        "\n",
        "print(f\"\\nTask E Complete!\")\n",
        "print(f\"Personalized top beer: {personalized_top3.iloc[0]['beer_name']}\")\n",
        "print(f\"Highest-rated top beer: {highest_rated_top3.index[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Task E, we compared our personalized recommendation method against a baseline that ignores similarity and sentiment, and instead simply selects the three highest-rated beers in the dataset.\n",
        "\n",
        "Our personalized model (Task B) identified Triple Shot, KBS, and Parabajava as the top recommendations for a customer wanting a beer that is chocolate, dark, and coffee. In contrast, the highest-rated baseline surfaced 10 Year Barleywine, M.J.K., and O.W.K., all of which had excellent ratings (average 4.94/5.0) but almost no mentions of the customer’s desired attributes. Additionally, there were no beers in common between these models.\n",
        "\n",
        "Overall, the results show that while the highest-rated products are objectively strong beers, they provide very poor relevance to the customer’s specific preferences. This emphasizes the value of incorporating similarity and sentiment signals: recommendations should balance quality with alignment to user requirements, and not just rely on ratings alone."
      ],
      "metadata": {
        "id": "YCx3_1xHM-f3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ByGsK0stHTy"
      },
      "source": [
        "Task F. Choose any 10 beers in your data. Now choose any one of them, and find the most similar beer\n",
        "(among the remaining 9). Explain your method and logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMlqSur0AI44",
        "outputId": "53fc3413-2865-4da2-9efc-d06a0318053a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task F: Direct Beer Content Similarity (Fixed)\n",
            "Valid beers found: 244\n",
            "Target: Saison Du Fermier\n",
            "Comparing against: 9 beers\n",
            "Comparison beers: ['Imperial Eclipse Stout - Elijah Craig (12 Year)', 'Bourbon Paradise', 'Samuel']...\n",
            "\n",
            "Most similar to 'Saison Du Fermier':\n",
            "  Fort Point Pale Ale - Mosaic Dry Hopped (similarity: 0.740)\n",
            "\n",
            "Top 5 most similar:\n",
            "  1. Fort Point Pale Ale - Mosaic Dry Hopped: 0.740\n",
            "  2. Samuel: 0.701\n",
            "  3. Leaner: 0.648\n",
            "  4. Heady Topper: 0.433\n",
            "  5. Imperial Eclipse Stout - Elijah Craig (12 Year): 0.131\n",
            "\n",
            "Task F Complete!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Task F: Simple Beer-to-Beer Content Similarity (Fixed)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "print(\"Task F: Direct Beer Content Similarity (Fixed)\")\n",
        "\n",
        "# Load and validate data\n",
        "df = pd.read_csv('beeradvocate_reviews.csv')\n",
        "\n",
        "# Clean and validate beer names\n",
        "def is_valid_beer_name(name):\n",
        "    if pd.isna(name) or not isinstance(name, str):\n",
        "        return False\n",
        "\n",
        "    # Filter out timestamps and metadata\n",
        "    invalid_patterns = [\n",
        "        r'\\d+\\s*(minutes?|hours?|days?|weeks?)\\s*ago',  # timestamps\n",
        "        r'character count:',  # system messages\n",
        "        r'thanks for the review',\n",
        "        r'report',\n",
        "        r'^\\s*$',  # empty strings\n",
        "        r'^[0-9\\.\\s]+$'  # just numbers\n",
        "    ]\n",
        "\n",
        "    name_lower = name.lower().strip()\n",
        "    for pattern in invalid_patterns:\n",
        "        if re.search(pattern, name_lower):\n",
        "            return False\n",
        "\n",
        "    return len(name.strip()) > 3  # Minimum reasonable length\n",
        "\n",
        "# Filter for valid beers\n",
        "df_clean = df[df['product_name'].apply(is_valid_beer_name)].copy()\n",
        "\n",
        "# Create beer profiles\n",
        "beer_profiles = df_clean.groupby('product_name').agg({\n",
        "    'product_review': lambda x: ' '.join(str(rev) for rev in x if pd.notna(rev) and len(str(rev)) > 20),\n",
        "    'user_rating': 'mean',\n",
        "    'product_name': 'count'\n",
        "}).rename(columns={'product_review': 'review_text', 'product_name': 'review_count'})\n",
        "\n",
        "# Filter beers with sufficient reviews and content\n",
        "min_reviews = 2\n",
        "min_text_length = 100\n",
        "beer_profiles = beer_profiles[\n",
        "    (beer_profiles['review_count'] >= min_reviews) &\n",
        "    (beer_profiles['review_text'].str.len() >= min_text_length)\n",
        "]\n",
        "\n",
        "print(f\"Valid beers found: {len(beer_profiles)}\")\n",
        "\n",
        "if len(beer_profiles) < 10:\n",
        "    print(\"Error: Not enough valid beers found\")\n",
        "    exit()\n",
        "\n",
        "# Select 10 beers\n",
        "selected_beers = beer_profiles.sample(n=10, random_state=None).index.tolist()\n",
        "target_beer = selected_beers[0]\n",
        "comparison_beers = selected_beers[1:]\n",
        "\n",
        "print(f\"Target: {target_beer}\")\n",
        "print(f\"Comparing against: {len(comparison_beers)} beers\")\n",
        "print(f\"Comparison beers: {comparison_beers[:3]}...\") # Show first 3\n",
        "\n",
        "# Compare review content directly\n",
        "target_text = beer_profiles.loc[target_beer, 'review_text']\n",
        "comparison_texts = [beer_profiles.loc[beer, 'review_text'] for beer in comparison_beers]\n",
        "\n",
        "# Clean text function\n",
        "def clean_review_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove common artifacts\n",
        "    text = re.sub(r'look smell taste \\d+[\\d\\.\\s]*', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'character count: \\d+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'thanks for the review.*?report', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'report\\.\\.\\.', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Clean all texts\n",
        "target_text_clean = clean_review_text(target_text)\n",
        "comparison_texts_clean = [clean_review_text(text) for text in comparison_texts]\n",
        "\n",
        "# Create TF-IDF vectors\n",
        "all_texts = [target_text_clean] + comparison_texts_clean\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=500,\n",
        "    stop_words='english',\n",
        "    min_df=1,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "try:\n",
        "    vectors = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n",
        "\n",
        "    # Find most similar\n",
        "    most_similar_idx = similarities.argmax()\n",
        "    most_similar_beer = comparison_beers[most_similar_idx]\n",
        "    highest_similarity = similarities[most_similar_idx]\n",
        "\n",
        "    print(f\"\\nMost similar to '{target_beer}':\")\n",
        "    print(f\"  {most_similar_beer} (similarity: {highest_similarity:.3f})\")\n",
        "\n",
        "    # Create results with similarities\n",
        "    results = list(zip(comparison_beers, similarities))\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(f\"\\nTop 5 most similar:\")\n",
        "    for i, (beer, sim) in enumerate(results[:5]):\n",
        "        print(f\"  {i+1}. {beer}: {sim:.3f}\")\n",
        "\n",
        "    print(f\"\\nTask F Complete!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in similarity calculation: {e}\")\n",
        "    print(\"Check if review texts contain sufficient content for comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Task F, we randomly sampled 10 beers from the dataset. From this set, we selected Saison Du Fermier as the target beer and compared it against the remaining 9.\n",
        "\n",
        "To perform the comparison, we aggregated all reviews for each beer into a single document, applied text cleaning to remove artifacts (e.g., timestamps, “thanks for the review”), and represented each beer’s text using a TF-IDF vectorization with unigrams and bigrams. We then measured similarity between the target and comparison beers using cosine similarity.\n",
        "\n",
        "The beer most similar to Saison Du Fermier was Fort Point Pale Ale – Mosaic Dry Hopped (similarity = 0.740). The next closest matches were Samuel (0.701), Leaner (0.648), Heady Topper (0.433), and Imperial Eclipse Stout – Elijah Craig (12 Year) (0.131).\n",
        "\n",
        "These results are intuitive: Saison Du Fermier and Fort Point Pale Ale – Mosaic Dry Hopped share common descriptors in their reviews around bright, dry, and hop-forward flavor profiles, while Samuel and Leaner also carry overlapping language that emphasizes balance and nuanced fermentation. Beers like Heady Topper appear with lower similarity, reflecting fewer shared descriptors but still some overlap in reviewer language."
      ],
      "metadata": {
        "id": "Bro8tY1JQyjA"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}